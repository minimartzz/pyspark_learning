{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Every Spark instance begins with `SparkSession`\n",
    "\n",
    "Content\n",
    "\n",
    "* PySpark Dataframes\n",
    "* Reading Datasets\n",
    "* Checking data types of columns (Schema)\n",
    "* Selecting columns and indexing\n",
    "* Summary of dataset\n",
    "* Adding and Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "  .builder \\\n",
    "  .appName('FCC Intro') \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-U3CLJRTM:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FCC Intro</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x25969244220>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Dataframes\n",
    "\n",
    "How to read various filetypes as Spark Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a CSV\n",
    "df_pyspark_csv = spark \\\n",
    "  .read \\\n",
    "  .option('header', 'true') \\\n",
    "  .csv('people.csv', inferSchema=True)\n",
    "\n",
    "# Alternative - pass options as variables\n",
    "df_pyspark_csv = spark.read.csv('people.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: int, job: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the variable will display the columnn headers\n",
    "df_pyspark_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the Schema\n",
    "df_pyspark_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Jorge|\n",
      "|  Bob|\n",
      "| John|\n",
      "| null|\n",
      "|Jacob|\n",
      "|  Kim|\n",
      "+-----+\n",
      "\n",
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Jorge| 30|\n",
      "|  Bob| 32|\n",
      "| John| 32|\n",
      "| null| 43|\n",
      "|Jacob| 23|\n",
      "|  Kim| 14|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select columns\n",
    "df_pyspark_csv.select('name').show()\n",
    "df_pyspark_csv.select(['name', 'age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+----------+\n",
      "| name|age|      job|double age|\n",
      "+-----+---+---------+----------+\n",
      "|Jorge| 30|Developer|        60|\n",
      "|  Bob| 32|Developer|        64|\n",
      "| John| 32|     null|        64|\n",
      "| null| 43|   Dancer|        86|\n",
      "|Jacob| 23|    Pilot|        46|\n",
      "|  Kim| 14|Marketing|        28|\n",
      "+-----+---+---------+----------+\n",
      "\n",
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "| John| 32|     null|\n",
      "| null| 43|   Dancer|\n",
      "|Jacob| 23|    Pilot|\n",
      "|  Kim| 14|Marketing|\n",
      "+-----+---+---------+\n",
      "\n",
      "+------+---+---------+----------+\n",
      "|rename|age|      job|double age|\n",
      "+------+---+---------+----------+\n",
      "| Jorge| 30|Developer|        60|\n",
      "|   Bob| 32|Developer|        64|\n",
      "|  John| 32|     null|        64|\n",
      "|  null| 43|   Dancer|        86|\n",
      "| Jacob| 23|    Pilot|        46|\n",
      "|   Kim| 14|Marketing|        28|\n",
      "+------+---+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding Columns\n",
    "df_pyspark_csv = df_pyspark_csv.withColumn('double age', df_pyspark_csv['age'] * 2)\n",
    "df_pyspark_csv.show()\n",
    "\n",
    "# Dropping Columns\n",
    "df_pyspark_csv.drop('double age').show()\n",
    "\n",
    "# Renaming Columns\n",
    "df_pyspark_csv.withColumnRenamed('name', 'rename').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Values\n",
    "\n",
    "* Dropping rows\n",
    "* Various parameter in dropping functionalities\n",
    "* handling missing values by mean, median, mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+\n",
      "|   name| age|       job|\n",
      "+-------+----+----------+\n",
      "|  Jorge|  30| Developer|\n",
      "|    Bob|  32| Developer|\n",
      "|   John|  32|      null|\n",
      "|   null|  43|    Dancer|\n",
      "|  Jacob|  23|     Pilot|\n",
      "|    Kim|  14| Marketing|\n",
      "|Johnson|null|     Coder|\n",
      "|  Frank|null|Consultant|\n",
      "+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark = spark.read.csv('people.csv', header=True, inferSchema=True)\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "|Jacob| 23|    Pilot|\n",
      "|  Kim| 14|Marketing|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop all na rows\n",
    "df_spark.na.drop().show()\n",
    "#   - how: any (as long as 1 is null) | all (all values in row is null)\n",
    "#   - thresh: int (min number to drop)\n",
    "#   - subset: column_name (only from specified column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---+--------------+\n",
      "|          name|age|           job|\n",
      "+--------------+---+--------------+\n",
      "|         Jorge| 30|     Developer|\n",
      "|           Bob| 32|     Developer|\n",
      "|          John| 32|Missing values|\n",
      "|Missing values| 43|        Dancer|\n",
      "|         Jacob| 23|         Pilot|\n",
      "|           Kim| 14|     Marketing|\n",
      "+--------------+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fill missing values\n",
    "df_spark.na.fill('Missing values').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using imputer to replace null values with mean of the column\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "  inputCols=['age'],\n",
    "  outputCols=[\"age_imputed\"],\n",
    ").setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+-----------+\n",
      "|   name| age|       job|age_imputed|\n",
      "+-------+----+----------+-----------+\n",
      "|  Jorge|  30| Developer|         30|\n",
      "|    Bob|  32| Developer|         32|\n",
      "|   John|  32|      null|         32|\n",
      "|   null|  43|    Dancer|         43|\n",
      "|  Jacob|  23|     Pilot|         23|\n",
      "|    Kim|  14| Marketing|         14|\n",
      "|Johnson|null|     Coder|         29|\n",
      "|  Frank|null|Consultant|         29|\n",
      "+-------+----+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_spark).transform(df_spark).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark DataFrames\n",
    "\n",
    "* Filter Operation\n",
    "* Logical Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jacob| 23|    Pilot|\n",
      "|  Kim| 14|Marketing|\n",
      "+-----+---+---------+\n",
      "\n",
      "+-----+---+-----+\n",
      "| name|age|  job|\n",
      "+-----+---+-----+\n",
      "|Jacob| 23|Pilot|\n",
      "+-----+---+-----+\n",
      "\n",
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "| John| 32|     null|\n",
      "| null| 43|   Dancer|\n",
      "|Jacob| 23|    Pilot|\n",
      "|  Kim| 14|Marketing|\n",
      "+-----+---+---------+\n",
      "\n",
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "| John| 32|     null|\n",
      "| null| 43|   Dancer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter Operations\n",
    "df_spark.filter(\"age < 30\").show()\n",
    "\n",
    "# AND\n",
    "df_spark.filter((df_spark['age'] < 30) & (df_spark['age'] > 20)).show()\n",
    "\n",
    "# OR\n",
    "df_spark.filter((df_spark['age'] < 30) | (df_spark['age'] > 20)).show()\n",
    "\n",
    "# NOT\n",
    "df_spark.filter(~(df_spark['age'] < 30)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "| name|\n",
      "+-----+\n",
      "|Jacob|\n",
      "|  Kim|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selecting a column\n",
    "df_spark.filter(\"age < 30\").select('name').show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby and Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|       job|count|\n",
      "+----------+-----+\n",
      "|      null|    1|\n",
      "|Consultant|    1|\n",
      "| Developer|    2|\n",
      "|     Coder|    1|\n",
      "| Marketing|    1|\n",
      "|    Dancer|    1|\n",
      "|     Pilot|    1|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby column and select the aggregate function\n",
    "df_spark.groupBy('job').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|       job|sum(age)|\n",
      "+----------+--------+\n",
      "|      null|      32|\n",
      "|Consultant|    null|\n",
      "| Developer|      62|\n",
      "|     Coder|    null|\n",
      "| Marketing|      14|\n",
      "|    Dancer|      43|\n",
      "|     Pilot|      23|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.groupBy('job').sum('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|    29.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate functions across entire dataframe\n",
    "df_spark.agg({\n",
    "  'age': 'mean'\n",
    "}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
