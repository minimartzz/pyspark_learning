{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Structured Streaming\n",
    "\n",
    "Scalable and fault-tolerant streaming engine built on the Spark SQL engine.\n",
    "\n",
    "__Features__\n",
    "\n",
    "* Expressing streaming computation the same way as on static data - engine will take care of running it incrementally and continuously\n",
    "* Use the Dataset/DataFrame API to express streaming functions\n",
    "* Computation is executed on same Spark SQL engine\n",
    "* System ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-ahead logs\n",
    "\n",
    "__How Structured Queries Work__\n",
    "\n",
    "1. Default - queries are processed using a _micro-batch processing engine_ which processes data streams as a series of small batch jobs, allowing it to achieve millisecond latency with exactly-once fault-tolerance guarentees\n",
    "2. Continuos Processing - achieve even beetter latency with at-least-once guarantees (faster, but less tolerant)\n",
    "\n",
    "Will explain the concepts used during micro-batch processing models, Continuous Processing models and the APIs used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "Maintaining a running word count of text data received from a data server listening on a TCP socket - streaming a word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StructuredNetworkWordCount\").getOrCreate()\n",
    "\n",
    "# Create streaming DataFrame that represents text data received from stream hosted at localhost, and\n",
    "# transforms the DataFrame to calculate word count\n",
    "\n",
    "lines = spark \\\n",
    "\t.readStream \\\n",
    "\t.format('socket') \\\n",
    "\t.option('host', 'localhost') \\\n",
    "\t.option('port', 9999) \\\n",
    "\t.load()\n",
    "words = lines.select(\n",
    "\texplode(\n",
    "\t\tsplit(lines.value, \" \")\n",
    "\t).alias('word')\n",
    ")\n",
    "word_counts = words.groupBy('word').count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`lines` DataFrame explained__\n",
    "\n",
    "Represents an unbounded table containing the streaming text data. Each row will represent a line in the streaming text data. It then splits the sentence into individual words and creates a new row for each unique word with an alias `word`. \n",
    "\n",
    "It then creates a DataFrame that groups the unqiue values in the Dataset and counts them. This is a streaming DataFrame which represents the running word counts of the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print complete every time the DataFrame is updated\n",
    "query = word_counts \\\n",
    "\t.writeStream \\\n",
    "\t.outputMode('complete') \\\n",
    "\t.format('console') \\\n",
    "\t.start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will await sentence data from the source (localhost:9999) and then process the data in the DataFrame. The below command will instantiate a Netcat data server to send the sentence data over. This is a small utility found in most Unix-like systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nc -lk 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Model\n",
    "\n",
    "Key idea is to treate a live data stream as a table that is being continuously appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
