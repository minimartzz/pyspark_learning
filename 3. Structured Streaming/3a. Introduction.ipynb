{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Structured Streaming\n",
    "\n",
    "Scalable and fault-tolerant streaming engine built on the Spark SQL engine.\n",
    "\n",
    "__Features__\n",
    "\n",
    "* Expressing streaming computation the same way as on static data - engine will take care of running it incrementally and continuously\n",
    "* Use the Dataset/DataFrame API to express streaming functions\n",
    "* Computation is executed on same Spark SQL engine\n",
    "* System ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-ahead logs\n",
    "\n",
    "__How Structured Queries Work__\n",
    "\n",
    "1. Default - queries are processed using a _micro-batch processing engine_ which processes data streams as a series of small batch jobs, allowing it to achieve millisecond latency with exactly-once fault-tolerance guarentees\n",
    "2. Continuos Processing - achieve even beetter latency with at-least-once guarantees (faster, but less tolerant)\n",
    "\n",
    "Will explain the concepts used during micro-batch processing models, Continuous Processing models and the APIs used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "\n",
    "Maintaining a running word count of text data received from a data server listening on a TCP socket - streaming a word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StructuredNetworkWordCount\").getOrCreate()\n",
    "\n",
    "# Create streaming DataFrame that represents text data received from stream hosted at localhost, and\n",
    "# transforms the DataFrame to calculate word count\n",
    "\n",
    "lines = spark \\\n",
    "\t.readStream \\\n",
    "\t.format('socket') \\\n",
    "\t.option('host', 'localhost') \\\n",
    "\t.option('port', 9999) \\\n",
    "\t.load()\n",
    "words = lines.select(\n",
    "\texplode(\n",
    "\t\tsplit(lines.value, \" \")\n",
    "\t).alias('word')\n",
    ")\n",
    "word_counts = words.groupBy('word').count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`lines` DataFrame explained__\n",
    "\n",
    "Represents an unbounded table containing the streaming text data. Each row will represent a line in the streaming text data. It then splits the sentence into individual words and creates a new row for each unique word with an alias `word`. \n",
    "\n",
    "It then creates a DataFrame that groups the unqiue values in the Dataset and counts them. This is a streaming DataFrame which represents the running word counts of the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print complete every time the DataFrame is updated\n",
    "query = word_counts \\\n",
    "\t.writeStream \\\n",
    "\t.outputMode('complete') \\\n",
    "\t.format('console') \\\n",
    "\t.start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code will await sentence data from the source (localhost:9999) and then process the data in the DataFrame. The below command will instantiate a Netcat data server to send the sentence data over. This is a small utility found in most Unix-like systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nc -lk 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Model\n",
    "\n",
    "Key idea is to treate a live data stream as a table that is being continuously appended. This means the new stream processing model is similar to a batch processing model. Computations are expressed as a batch-like query and Spark runs it as an _incremental_ query on the _unbounded_ input table.\n",
    "\n",
    "## Basic Concepts\n",
    "\n",
    "Every data item that is arriving on the stream is like a new row being added to the unbounded Input Table. A query on the input will generate the Results Table which will display the current state of the Input Table at that instance. When new rows are appended to the Input Table, it will update the Results Table and changes should be written to an external sink.\n",
    "\n",
    "![Updating the Results Table](https://spark.apache.org/docs/latest/img/structured-streaming-model.png)\n",
    "\n",
    "The \"Output\" is defined as what gets written out to the external storage. The output can be defined in different modes:\n",
    "\n",
    "1. __Complete Mode__ - The entire updated Results Table is written to the external storage. Depends on the storage connector to decide how to handle the writing of the entire table.\n",
    "2. __Append Mode__ - Only append new rows appended in the Results Table since the last trigger. This is applicable only to queries where the previous instances are not expected to have changed.\n",
    "3. __Update Mode__ - Only rows that were updated since the previous trigger will be written. This is different from Complete Mode in that only the rows that have been changed will be written and not all the rows. If there are no changes to previous entries, this is equivalent to Append Mode.\n",
    "\n",
    "_Note: Each mode is applicable on certain types of queries_\n",
    "\n",
    "In the Example above `lines` is the Input Table and the final `word_counts` will be the Results Table. Spark will continuously check for new data from the socket connection and run an \"incremental\" query that combines previous running counts with new data. DataFrame to generate the Results table queries the same way as a static table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Event-time and Late Data\n",
    "\n",
    "In this model, Spark is responsible for updating the Result Table when there is new data, thus relieving users from reasoning about fault-tolerance and data consistency.\n",
    "\n",
    "Event-time is time embedded in the data itself (usually when the data was generated). In this model, each event is considered a row in the table and Event-time is a column. This allows window-based aggregation, where a grouping and aggregation can be done on the Event-time column - Each window is a group and each row can belong to multiple windows. Therefore, queries can be defined consistently.\n",
    "\n",
    "Also, since Spark is updaring the Results table on a regular basis, it can update the old aggregates whenever \"late\" data has arrived. _Watermarking_ specify the threshold of late data, and allows engine to clean up old state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fault Tolerance Semantics\n",
    "\n",
    "* Streaming sources, sinks and execution engine track the exact progress of the processing to handle any kind of failure by restarting or reprocessing\n",
    "* Every streaming source is assumed to have an offset to track the read position in the stream\n",
    "* Checkpointing and write-ahead logs are used to track the offset range of the data processed\n",
    "* Streaming sinks are idempotent and used for handling reprocessing\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
