{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. API using Datasets and DataFrames\n",
    "\n",
    "Can use `SparkSession` as the entrypoint to create streaming DataFrames/Datasets.\n",
    "\n",
    "## Creating streaming Datasets and DataFrames\n",
    "\n",
    "* `DataStreamReader` -- creates a streaming DataFrame\n",
    "* `SparkSession.readStream()` -- returns the stream DataFrame\n",
    "\n",
    "### Input Sources\n",
    "\n",
    "* __File Source__ - Reads files written in a directory as a stream of data. Files are processed in order of _modification time_. Files must be atomically placed in the given directory, which in most file systems is a file move operation\n",
    "\t* supports glob paths, but not multiple comma-separated paths/globs\n",
    "\t* archiving or deleting completed files will introduce overhead which will slow down the micro-batch processing speed. This option will also reduce the cost to list source files which can be expensive\n",
    "\t* source path should not be used from multiple soruces or queries. It shouldn't match any files in output directory or stream sink\n",
    "\t* delete and move actions are best effort. Failing to delete or move will not fail the streaming query \n",
    "* __Kafka Source__ - Reads data from a Kafka source\n",
    "* __Socket Source (for testing)__ - Reads UTF8 text data from a socket connection. The listening socket is at the driver. Note that this should only be used for testing, exactly-once fault tolerance is not guaranteed.\n",
    "\t* host and port numbers must be provided\n",
    "* __Rate Source (for testing)__ - Generates data a specified number of rows per second, each output row contains a timestamp (Timestamp data type) containing time of message dispatch and value (Long type) containing the message count. Source is intended for testing and benchmarking\n",
    "\t* source will try to reach `rowsPerSecond` parameter\n",
    "\t* `numPartitions` can be tweaked to help reach desired speed\n",
    "* __Rate Per Micro-Batch source (for testing)__ - PRovides a specified number of rows per micro-batch. Difference between above is it provides a consistent set of inputs per row in each batch regardless of query execution (i.e batch 0 - 0-99, batch 1 - 100-199, ...). Both timestamp and value are also  provided. Source is intended for testing and benchmarking \n",
    "\t* similar comments to above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"API using Datasets and Datastructures\").getOrCreate()\n",
    "\n",
    "# Example 1 - socket stream\n",
    "df_socket = spark \\\n",
    "\t.readStream \\\n",
    "\t.format('socket') \\\n",
    "\t.option('host', 'localhost') \\\n",
    "\t.option('port', 9999) \\\n",
    "\t.load()\n",
    "\n",
    "df_socket.isStreaming() # Returns True for DataFrames that have streaming sources\n",
    "\n",
    "df_socket.printSchema()\n",
    "\n",
    "# Example 2 - Read all the csv\n",
    "userSchema = StructType().add('name', 'string').add('age', 'integer')\n",
    "df_csv = spark \\\n",
    "\t.readStream \\\n",
    "\t.option('sep', ';') \\\n",
    "\t.schema(userSchema) \\\n",
    "\t.csv('path/to/directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some operations (e.g `map`, `flatMap`) require type to be known at compile time. Above only checks the type at runtime. Untyped streaming DataFrames can be transformed to typed ones using the same method as static DataFrames.\n",
    "\n",
    "_Refer to 2a. Getting Started - Programmatically specifying the schema to know how to do this_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema inference and parition streaming DataFrames/Datasets\n",
    "\n",
    "File based sources require schema specification for Structured Streaming. Can be enabled using `spark.sql.streaming.schemaInference` to true for other ad-hoc use cases.\n",
    "\n",
    "Partition discovery does occur when subdirectories that are named `/key=value/` are present and listing will automatically recurse into these directories. The directories that make up the partitioning scheme must be present when the query starts and must remain static (means the partition columns cannot change in scope)\n",
    "\n",
    "For example, it is okay to add `/data/year=2016/` when `/data/year=2015/` was present, but it is invalid to change the partitioning column (i.e. by creating the directory `/data/date=2016-04-17/`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations on streaming DataFrames/Datasets\n",
    "\n",
    "Both untyped, SQL-like operations (SELECT, WHERE, GROUPBY) and RDD-like operations (map, filter, flatMap) are applicable\n",
    "\n",
    "## Basic Operations - Selection, Projection, Aggregation\n",
    "\n",
    "Most common operations on DataFrames/Datasets are supported for streaming versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\MA Stuff\\Random\\3. Structured Streaming\\3b. API using Datasets and DataFrames.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=0'>1</a>\u001b[0m \u001b[39m# streaming dataframe with IOT device data with schema\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=1'>2</a>\u001b[0m \u001b[39m# {device: string, deviceType: string, signal: double, time: TimeStamp}\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=2'>3</a>\u001b[0m df \u001b[39m=\u001b[39m spark \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=3'>4</a>\u001b[0m \t\u001b[39m.\u001b[39mreadStream \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=4'>5</a>\u001b[0m \t\u001b[39m.\u001b[39mformat(\u001b[39m'\u001b[39m\u001b[39msocket\u001b[39m\u001b[39m'\u001b[39m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=5'>6</a>\u001b[0m \t\u001b[39m.\u001b[39moption(\u001b[39m'\u001b[39m\u001b[39mhost\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlocalhost\u001b[39m\u001b[39m'\u001b[39m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=6'>7</a>\u001b[0m \t\u001b[39m.\u001b[39moption(\u001b[39m'\u001b[39m\u001b[39mport\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m9999\u001b[39m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=7'>8</a>\u001b[0m \t\u001b[39m.\u001b[39mload()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=9'>10</a>\u001b[0m \u001b[39m# Select the devices which have signal more than 10\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=10'>11</a>\u001b[0m df\u001b[39m.\u001b[39mselect(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mwhere(\u001b[39m\"\u001b[39m\u001b[39msignal > 10\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# streaming dataframe with IOT device data with schema\n",
    "# {device: string, deviceType: string, signal: double, time: TimeStamp}\n",
    "df = spark \\\n",
    "\t.readStream \\\n",
    "\t.format('socket') \\\n",
    "\t.option('host', 'localhost') \\\n",
    "\t.option('port', 9999) \\\n",
    "\t.load()\n",
    "\n",
    "# Select the devices which have signal more than 10\n",
    "df.select('device').where(\"signal > 10\")\n",
    "\n",
    "# Running count of the number of updates for each device\n",
    "df.groupBy(\"deviceType\").count()\n",
    "\n",
    "# Register streaming DataFrame as a temporary view and apply SQL commands\n",
    "df.createOrReplaceTempView('updates')\n",
    "spark.sql(\"SELECT count(*) FROM updates\")\n",
    "\n",
    "# Identify whether DataFrame has streaming data\n",
    "df.isStreaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Operations on Event Time\n",
    "\n",
    "Runs similar to grouped aggregations. Aggregate values are maintained for the rows which have Event-time that gall within the window.\n",
    "\n",
    "Example: Want to count the words within a 10min window, which updates every 5mins (e.g 12:00-12:10, 12:05-12:15, 12:10-12:20, ...). If a word comes in at 12:07 it should update values in 2 windows. Counts will be indexed by both, the grouping key and the window. Illustrated below:\n",
    "\n",
    "![streaming window example](https://spark.apache.org/docs/latest/img/structured-streaming-window.png)\n",
    "\n",
    "* `window(<column>, <window_interval>, <sliding_interval>)` - create a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "# schema - {timestamp: Timestap, word: String}\n",
    "words = spark \\\n",
    "\t.readStream \\\n",
    "\t.format('socket') \\\n",
    "\t.option('host', 'localhost') \\\n",
    "\t.option('port', 9999) \\\n",
    "\t.load()\n",
    "\n",
    "# Group the data by window and word and compute the count to each group\n",
    "windowed_count = words.groupBy(\n",
    "\twindow(words.timestamp, '10 minutes', '5 minutes'),\n",
    "\twords.word\n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Late Data and Watermarking\n",
    "\n",
    "Structured Streaming can maintain the intermediate state for the partial aggregates for a long period of time such that late data can update aggregates of old windows correctly\n",
    "\n",
    "However, system needs to bound the amount of intermediate in-memory state it accumulates i.e needs to know when an old aggregate can be dropped from in-memory state.\n",
    "\n",
    "_Watermarking_ - specify the event time column and threshold on how late the data is expected to be in terms of event time. $(max\\ event\\ time\\ seen\\ by\\ engine - late\\ threshold > T)$ where $T$ is the specific window ending at time $T$. Data later than the threshold will be dropped (watermark is the latest timestamp - threshold)\n",
    "\n",
    "* `.withWatermark(<column>, <threshold>)` - timeframe for late data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema - {timestamp: Timestap, word: String}\n",
    "words = spark \\\n",
    "\t.readStream \\\n",
    "\t.format('socket') \\\n",
    "\t.option('host', 'localhost') \\\n",
    "\t.option('port', 9999) \\\n",
    "\t.load()\n",
    "\n",
    "windowed_counts = words \\\n",
    "\t.withWatermark('timestamp', '10 minutes') \\\n",
    "\t.groupBy(\n",
    "\t\twindow(words.timestamp, '10 minutes', '5 minutes'),\n",
    "\t\twords.word\n",
    "\t) \\\n",
    "\t.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how watermarking will work\n",
    "\n",
    "![watermark example](https://spark.apache.org/docs/latest/img/structured-streaming-watermark-update-mode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of time windows\n",
    "\n",
    "1. __Tumbling Windows__ - series of fixed-sized non-overlapping and contiguous time intervals. An input can only be bound to a single window\n",
    "2. __Sliding Windows__ - \"fixed-sized\", but windows can overlap if the duration of slide is smaller than duration of window. Inputs can be bound to multiple windows\n",
    "3. __Sesion Windows__ - A session window starts when an input is received, and will extend if a new input is received within the specified gap duration. If no entry is received, then the window closes and awaits the next input to restart a new window. They are therefore dynamic in size of the window length\n",
    "\t* functions/expressions can be used to specify the gap duration dynamically based on the input row. With fynamic gap durations, the closing of a session window does not depend on the latest input anymore, it's range is the union of all events' range\n",
    "\n",
    "1 and 2 use `window()`, 3 uses `session_window()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import session_window\n",
    "\n",
    "events = spark \\\n",
    "\t.readStream \\\n",
    "\t.format('socket') \\\n",
    "\t.option('host', 'localhost') \\\n",
    "\t.option('port', 9999) \\\n",
    "\t.load()\n",
    "\n",
    "# Static session_window\n",
    "static_sessionized_counts = events \\\n",
    "\t.withWatermark('timestamp', '10 minutes') \\\n",
    "\t.groupBy(\n",
    "\t\tsession_window(events.timestamp, '5 minutes'),\n",
    "\t\tevents.userId\n",
    "\t).count()\n",
    "\n",
    "# Dynamic session_window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "session_window = session_window(\n",
    "\tevents.timestamp,\n",
    "\tF.when(events.userId == 'user1', '5 seconds').when(events.userId == 'user2', '20 seconds').otherwise('5 minutes')\n",
    ")\n",
    "\n",
    "dynamic_sessionzied_counts = events \\\n",
    "\t.withWatermark('timestamp', '10 minutes') \\\n",
    "\t.groupBy(\n",
    "\t\tsession_window,\n",
    "\t\tevents.userId\n",
    "\t).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditions for watermarking to clean aggregation state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be53bca86783c5ebc1c4a1f19d1efb4121d14fd473a368b8237af582f8e0193e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
