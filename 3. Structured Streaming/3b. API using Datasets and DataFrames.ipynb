{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. API using Datasets and DataFrames\n",
    "\n",
    "Can use `SparkSession` as the entrypoint to create streaming DataFrames/Datasets.\n",
    "\n",
    "## Creating streaming Datasets and DataFrames\n",
    "\n",
    "* `DataStreamReader` -- creates a streaming DataFrame\n",
    "* `SparkSession.readStream()` -- returns the stream DataFrame\n",
    "\n",
    "### Input Sources\n",
    "\n",
    "* __File Source__ - Reads files written in a directory as a stream of data. Files are processed in order of _modification time_. Files must be atomically placed in the given directory, which in most file systems is a file move operation\n",
    "\t* supports glob paths, but not multiple comma-separated paths/globs\n",
    "\t* archiving or deleting completed files will introduce overhead which will slow down the micro-batch processing speed. This option will also reduce the cost to list source files which can be expensive\n",
    "\t* source path should not be used from multiple soruces or queries. It shouldn't match any files in output directory or stream sink\n",
    "\t* delete and move actions are best effort. Failing to delete or move will not fail the streaming query \n",
    "* __Kafka Source__ - Reads data from a Kafka source\n",
    "* __Socket Source (for testing)__ - Reads UTF8 text data from a socket connection. The listening socket is at the driver. Note that this should only be used for testing, exactly-once fault tolerance is not guaranteed.\n",
    "\t* host and port numbers must be provided\n",
    "* __Rate Source (for testing)__ - Generates data a specified number of rows per second, each output row contains a timestamp (Timestamp data type) containing time of message dispatch and value (Long type) containing the message count. Source is intended for testing and benchmarking\n",
    "\t* source will try to reach `rowsPerSecond` parameter\n",
    "\t* `numPartitions` can be tweaked to help reach desired speed\n",
    "* __Rate Per Micro-Batch source (for testing)__ - PRovides a specified number of rows per micro-batch. Difference between above is it provides a consistent set of inputs per row in each batch regardless of query execution (i.e batch 0 - 0-99, batch 1 - 100-199, ...). Both timestamp and value are also  provided. Source is intended for testing and benchmarking \n",
    "\t* similar comments to above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"API using Datasets and Datastructures\").getOrCreate()\n",
    "\n",
    "# Example 1 - socket stream\n",
    "df_socket = spark \\\n",
    "\t.readStream \\\n",
    "\t.format('socket') \\\n",
    "\t.option('host', 'localhost') \\\n",
    "\t.option('port', 9999) \\\n",
    "\t.load()\n",
    "\n",
    "df_socket.isStreaming() # Returns True for DataFrames that have streaming sources\n",
    "\n",
    "df_socket.printSchema()\n",
    "\n",
    "# Example 2 - Read all the csv\n",
    "userSchema = StructType().add('name', 'string').add('age', 'integer')\n",
    "df_csv = spark \\\n",
    "\t.readStream \\\n",
    "\t.option('sep', ';') \\\n",
    "\t.schema(userSchema) \\\n",
    "\t.csv('path/to/directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some operations (e.g `map`, `flatMap`) require type to be known at compile time. Above only checks the type at runtime. Untyped streaming DataFrames can be transformed to typed ones using the same method as static DataFrames.\n",
    "\n",
    "_Refer to 2a. Getting Started - Programmatically specifying the schema to know how to do this_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema inference and parition streaming DataFrames/Datasets\n",
    "\n",
    "File based sources require schema specification for Structured Streaming. Can be enabled using `spark.sql.streaming.schemaInference` to true for other ad-hoc use cases.\n",
    "\n",
    "Partition discovery does occur when subdirectories that are named `/key=value/` are present and listing will automatically recurse into these directories. The directories that make up the partitioning scheme must be present when the query starts and must remain static (means the partition columns cannot change in scope)\n",
    "\n",
    "For example, it is okay to add `/data/year=2016/` when `/data/year=2015/` was present, but it is invalid to change the partitioning column (i.e. by creating the directory `/data/date=2016-04-17/`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations on streaming DataFrames/Datasets\n",
    "\n",
    "Both untyped, SQL-like operations (SELECT, WHERE, GROUPBY) and RDD-like operations (map, filter, flatMap) are applicable\n",
    "\n",
    "## Basic Operations - Selection, Projection, Aggregation\n",
    "\n",
    "Most common operations on DataFrames/Datasets are supported for streaming versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\MA Stuff\\Random\\3. Structured Streaming\\3b. API using Datasets and DataFrames.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=0'>1</a>\u001b[0m \u001b[39m# streaming dataframe with IOT device data with schema\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=1'>2</a>\u001b[0m \u001b[39m# {device: string, deviceType: string, signal: double, time: TimeStamp}\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=2'>3</a>\u001b[0m df \u001b[39m=\u001b[39m spark \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=3'>4</a>\u001b[0m \t\u001b[39m.\u001b[39mreadStream \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=4'>5</a>\u001b[0m \t\u001b[39m.\u001b[39mformat(\u001b[39m'\u001b[39m\u001b[39msocket\u001b[39m\u001b[39m'\u001b[39m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=5'>6</a>\u001b[0m \t\u001b[39m.\u001b[39moption(\u001b[39m'\u001b[39m\u001b[39mhost\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlocalhost\u001b[39m\u001b[39m'\u001b[39m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=6'>7</a>\u001b[0m \t\u001b[39m.\u001b[39moption(\u001b[39m'\u001b[39m\u001b[39mport\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m9999\u001b[39m) \\\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=7'>8</a>\u001b[0m \t\u001b[39m.\u001b[39mload()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=9'>10</a>\u001b[0m \u001b[39m# Select the devices which have signal more than 10\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/3.%20Structured%20Streaming/3b.%20API%20using%20Datasets%20and%20DataFrames.ipynb#ch0000005?line=10'>11</a>\u001b[0m df\u001b[39m.\u001b[39mselect(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mwhere(\u001b[39m\"\u001b[39m\u001b[39msignal > 10\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# streaming dataframe with IOT device data with schema\n",
    "# {device: string, deviceType: string, signal: double, time: TimeStamp}\n",
    "df = spark \\\n",
    "\t.readStream \\\n",
    "\t.format('socket') \\\n",
    "\t.option('host', 'localhost') \\\n",
    "\t.option('port', 9999) \\\n",
    "\t.load()\n",
    "\n",
    "# Select the devices which have signal more than 10\n",
    "df.select('device').where(\"signal > 10\")\n",
    "\n",
    "# Running count of the number of updates for each device\n",
    "df.groupBy(\"deviceType\").count()\n",
    "\n",
    "# Register streaming DataFrame as a temporary view and apply SQL commands\n",
    "df.createOrReplaceTempView('updates')\n",
    "spark.sql(\"SELECT count(*) FROM updates\")\n",
    "\n",
    "# Identify whether DataFrame has streaming data\n",
    "df.isStreaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Window Operations on Event Time\n",
    "\n",
    "Runs similar to grouped aggregations. Aggregate values are maintained for the rows which have Event-time that gall within the window.\n",
    "\n",
    "Example: Want to count the words within a 10min window, which updates every 5mins (e.g 12:00-12:10, 12:05-12:15, 12:10-12:20, ...). If a word comes in at 12:07 it should update values in 2 windows. Counts will be indexed by both, the grouping key and the window. Illustrated below:\n",
    "\n",
    "![streaming window example](https://spark.apache.org/docs/latest/img/structured-streaming-window.png)\n",
    "\n",
    "* `window(<column>, <window_interval>, <sliding_interval>)` - create a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "# schema - {timestamp: Timestap, word: String}\n",
    "words = spark \\\n",
    "\t.readStream \\\n",
    "\t.format('socket') \\\n",
    "\t.option('host', 'localhost') \\\n",
    "\t.option('port', 9999) \\\n",
    "\t.load()\n",
    "\n",
    "# Group the data by window and word and compute the count to each group\n",
    "windowed_count = words.groupBy(\n",
    "\twindow(words.timestamp, '10 minutes', '5 minutes'),\n",
    "\twords.word\n",
    ").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Late Data and Watermarking\n",
    "\n",
    "Structured Streaming can maintain the intermediate state for the partial aggregates for a long period of time such that late data can update aggregates of old windows correctly\n",
    "\n",
    "However, system needs to bound the amount of intermediate in-memory state it accumulates i.e needs to know when an old aggregate can be dropped from in-memory state.\n",
    "\n",
    "_Watermarking_ - specify the event time column and threshold on how late the data is expected to be in terms of event time. $(max\\ event\\ time\\ seen\\ by\\ engine - late\\ threshold > T)$ where $T$ is the specific window ending at time $T$. Data later than the threshold will be dropped (watermark is the latest timestamp - threshold)\n",
    "\n",
    "* `.withWatermark(<column>, <threshold>)` - timeframe for late data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema - {timestamp: Timestap, word: String}\n",
    "words = spark \\\n",
    "\t.readStream \\\n",
    "\t.format('socket') \\\n",
    "\t.option('host', 'localhost') \\\n",
    "\t.option('port', 9999) \\\n",
    "\t.load()\n",
    "\n",
    "windowed_counts = words \\\n",
    "\t.withWatermark('timestamp', '10 minutes') \\\n",
    "\t.groupBy(\n",
    "\t\twindow(words.timestamp, '10 minutes', '5 minutes'),\n",
    "\t\twords.word\n",
    "\t) \\\n",
    "\t.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of how watermarking will work\n",
    "\n",
    "![watermark example](https://spark.apache.org/docs/latest/img/structured-streaming-watermark-update-mode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of time windows\n",
    "\n",
    "1. __Tumbling Windows__ - series of fixed-sized non-overlapping and contiguous time intervals. An input can only be bound to a single window\n",
    "2. __Sliding Windows__ - \"fixed-sized\", but windows can overlap if the duration of slide is smaller than duration of window. Inputs can be bound to multiple windows\n",
    "3. __Sesion Windows__ - A session window starts when an input is received, and will extend if a new input is received within the specified gap duration. If no entry is received, then the window closes and awaits the next input to restart a new window. They are therefore dynamic in size of the window length\n",
    "\t* functions/expressions can be used to specify the gap duration dynamically based on the input row. With fynamic gap durations, the closing of a session window does not depend on the latest input anymore, it's range is the union of all events' range\n",
    "\n",
    "1 and 2 use `window()`, 3 uses `session_window()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import session_window\n",
    "\n",
    "events = spark \\\n",
    "\t.readStream \\\n",
    "\t.format('socket') \\\n",
    "\t.option('host', 'localhost') \\\n",
    "\t.option('port', 9999) \\\n",
    "\t.load()\n",
    "\n",
    "# Static session_window\n",
    "static_sessionized_counts = events \\\n",
    "\t.withWatermark('timestamp', '10 minutes') \\\n",
    "\t.groupBy(\n",
    "\t\tsession_window(events.timestamp, '5 minutes'),\n",
    "\t\tevents.userId\n",
    "\t).count()\n",
    "\n",
    "# Dynamic session_window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "session_window = session_window(\n",
    "\tevents.timestamp,\n",
    "\tF.when(events.userId == 'user1', '5 seconds').when(events.userId == 'user2', '20 seconds').otherwise('5 minutes')\n",
    ")\n",
    "\n",
    "dynamic_sessionzied_counts = events \\\n",
    "\t.withWatermark('timestamp', '10 minutes') \\\n",
    "\t.groupBy(\n",
    "\t\tsession_window,\n",
    "\t\tevents.userId\n",
    "\t).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditions for watermarking to clean aggregation state\n",
    "\n",
    "Following conditions must be satisfied for the watermarking to clean the stat in aggregation queries:\n",
    "\n",
    "1. Output mode must be Append or Update\n",
    "2. Aggreation must have either have the event-time column or a `window` on the event-time column\n",
    "3. `withWatermark` must be called on the same column as the timestamp column used in the aggregation\n",
    "4. `withWatermark` must be called before the aggregation for the watermark details to be used\n",
    "\n",
    "## Semantic Guarantees of Aggregation with Watermarking\n",
    "\n",
    "* A watermark delay of \"2 hours\" gurantees that any data that is received less than 2 hours after the latest data processed will always be aggregated\n",
    "* Gurantee is strictly in one direction. Data delayed more than 2 hours is not guranteed to be dropped; may or may not be aggregated - more delayed the data is the less likely it will be aggregated\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Operations\n",
    "\n",
    "Structured Streaming supports joins between streaming DataFrames, static DataFrames, and other streaming DataFrames. The result of the join is incremental (similar to aggregation queries). The result of the join with a streaming DataFrame is the same as if it were a static DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream-static Joins\n",
    "\n",
    "Supported joins (inner joins and some types of outer joins) between a streaming and static DataFrame. Stream-static joins are not stateful, so no state management is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"joins\").getOrCreate()\n",
    "\n",
    "df_static = spark.read.json('players.json') # file does not exist\n",
    "df_stream = spark \\\n",
    "\t.readStream \\\n",
    "\t.format('socket') \\\n",
    "\t.option('host', 'localhost') \\\n",
    "\t.option('port', 9999) \\\n",
    "\t.load()\n",
    "\n",
    "df_stream.join(df_static, 'type') # inner equi-join with a static DF\n",
    "df_stream.join(df_static, 'type', 'left_outer') # left outer join with a static DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream-stream Joins\n",
    "\n",
    "Both dataFrames are still incomplete, meaning that current rows on the first DataFrame can join with rows that are yet to enter on the second DataFrame. \n",
    "\n",
    "Hence, for both streams, PySpark will buffer past input as streaming state, so that every future input can still be matched with a part input and results generated accordingly. Furthermore, it's still able to handle late and out-of-order data based on watermakrs.\n",
    "\n",
    "Below are a few types of stream-stream joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Joins with optional Watermarking\n",
    "\n",
    "Inner joins on any kind of columns along with any kind of join conditions are supported. As stream runs, the streaming state will continue to increase, so to avoid unbounded state, have to define additional join conditions such that old inputs cannot match with new inputs indefinitely, essentially clearing them from the state. \n",
    "\n",
    "The follow steps are required:\n",
    "\n",
    "1. Define watermark delays on both inputs such that the engine knows how delayed the inputs can get\n",
    "2. Define a constraint on event-time across 2 inputs such that the engine will know when old rows of 1 input will no longer match with new inputes of the other input. Can defined in the following:\n",
    "\t* Time range join conditions (`... JOIN ON leftTime BETWEEN rightTime and rightTime + INTERVAL 1 HOUR`)\n",
    "\t* Join on event-time windows (`... JOIN ON leftTimeWindow = rightTimeWindow`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g matching advertisement air-times with customer clicks to correlate monetizable clicks\n",
    "# ad impressions watermark = 2hrs | clicks watermark = 3hrs\n",
    "# join time range = 1hr after impression is made\n",
    "\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "impressions = spark.readStream.format('socket').option('host', 'localhost').option('port', 9999).load()\n",
    "clicks = spark.readStream.format('socket').option('host', 'localhost').option('port', 9999).load()\n",
    "\n",
    "# Apply watermarks on event-time columns\n",
    "impressions_watermarked = impressions.withWatermark('impressionTime', '2 hours')\n",
    "clicks_watermarked = impressions.withWatermark('clickTime', '3 hours')\n",
    "\n",
    "# Join with event-time constraints\n",
    "impressions_watermarked.join(\n",
    "\tclicks_watermarked,\n",
    "\texpr(\n",
    "\t\t'''\n",
    "\t\tclickAdId = impressionAdId AND\n",
    "\t\tclickTime >= impreesionTime AND\n",
    "\t\tclickTime <= impressionTime + interval 1 hour\t\t\n",
    "\t\t'''\n",
    "\t)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer Joins with Watermarking\n",
    "\n",
    "The watermark + event-tme constraints must be specified for outer joins. Engine has to know when an input row is not going to match with anything else in the future.\n",
    "\n",
    "Outer join queries will look like inner joins except it will specify an additional parameter to indicate it is an outer join\n",
    "\n",
    "__Caveats__\n",
    "\n",
    "* The outer NULL result will be generated with a _delay that matches the watermark delay and the time range condition_. This is to ensure that all delayed data is accounted for before returning the result\n",
    "* If any of the 2 input streams being joined does not receive data for a while, the outer (both cases left and right) output may get delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impressions_watermarked.join(\n",
    "\tclicks_watermarked,\n",
    "\texpr(\n",
    "\t\t'''\n",
    "\t\tclickAdId = impressionAdId AND\n",
    "\t\tclickTime >= impreesionTime AND\n",
    "\t\tclickTime <= impressionTime + interval 1 hour\t\t\n",
    "\t\t'''\n",
    "\t),\n",
    "\t'leftOuter' # can be 'inner', 'leftOuter', 'rightOuter', 'leftSemi'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi joins with Watermarking\n",
    "\n",
    "Watermark + event-time constraint must be specified for semi joins. They have the same guarantees as inner joins regarding watermark delays and whether data gets dropped or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Details on Joins\n",
    "\n",
    "* Joins can be cascaded, `df1.join(df2, ...).join(df3, ...).`\n",
    "* Only use joins when the query is in Append output mode\n",
    "* Cannot use other non-map-like operations before joins (e.g use streaming aggregations before joins)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Deduplication\n",
    "\n",
    "Deplication is removing repeated instances of data to decrease storage capacity requirements.\n",
    "\n",
    "Can deduplicate records in the data using a unique identifier in the events. Query will store the necessary data from the previous records to perform the duplication check. Can use deduplication with or without watermarking.\n",
    "\n",
    "* _With watermark_: If there is an upper bound on how late a duplicate record may arrive, then you can define a watermark on an event time column and deduplicate using both the guid and the event time columns. The query will use the watermark to remove old state data from past records that are not expected to get any duplicates any more. This bounds the amount of the state the query has to maintain.\n",
    "* _Without watermark_: no bounds on when a duplicate record may arrive, the query stores data from all past records as state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stream = spark.readStream.format('socket').option('host', 'localhost').option('port', 9999).load()\n",
    "\n",
    "# Without watermark using guid column\n",
    "df_stream.dropDuplicates('guid')\n",
    "\n",
    "# With watermark using guid and eventTime columns\n",
    "df_stream \\\n",
    "\t.withWatermark('eventTime', '10 seconds') \\\n",
    "\t.dropDuplicates('guid', 'eventTime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Policy for handling multiple watermarks\n",
    "\n",
    "A global watermark is created when multiple watermarked streams are used and joined together. By defaul the minimum watermark time is chosen to ensure that no data is accidentally dropped if 1 stream falls behind. Global watermark will move at the pace of the slowest stream and query output will be delayed accordingly \n",
    "\n",
    "Able to change this global watermark policy using `spark.sql.streaming.multipleWatermarkPolicy`. However, consider the side effects before changing this\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbitrary Stateful Operations\n",
    "\n",
    "For more advanced stateful operations, use sessionization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupported Operations\n",
    "\n",
    "see documentation for list of unsupported operations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitation of Global Watermark\n",
    "\n",
    "In Append mode, if a stateful operation emits rows older than the current watermark + allowed late record delay, they are considered \"late rows\" in downstream stateful operation(s). These rows may be discarded and they can potentially cause a correctness issue.\n",
    "\n",
    "Spark has 2 ways to check the number of late rows on stateful operators:\n",
    "\n",
    "1. On Spark UI: check the metrics in stateful oeprator nodes in the query execution details page in SQL tab\n",
    "2. On Streaming Query Listener: check \"numRowsDroppedByWatermark\" in \"stateOperators\" in QueryProcessEvent\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Store\n",
    "\n",
    "State store is a versioned key-value store that provides read and write operations. In Structured-Streaming, it is used to handle the stateful operations across batches. There are 2 built-in state store providers, but users can implement their own sate store provider.\n",
    "\n",
    "1. HDFS state store provider\n",
    "2. RocksDB state store implementation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting Streaming Queries\n",
    "\n",
    "Once the final result of the DataFrame has been defined, the streaming query needs to be started. Use `DataStreamWriter` returned through `Dataset.writeStream()` with the following parameters.\n",
    "\n",
    "* `DataStreamWriter()` - start the stream query\n",
    "\t* _Details of output sink_ -- data format, location, etc.\n",
    "\t* _Output mode_ (default append) -- specify what gets written to the output sink\n",
    "\t* _Query name_ (optional) -- specify a unique name of the query for identification\n",
    "\t* _Trigger interval_ (optional) -- specify trigger interval. If not system will check for avilability of new data once previous proccessing has been completed\n",
    "\t\t* if trigger time is missed, it will start the next process immediately once done\n",
    "\t* _Checkpoint location_ -- specify the location where the system will write all checkpoint information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Modes\n",
    "\n",
    "Append (default), Complete, Update. Refer to documentation to see which modes are compatible with which query type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Sinks\n",
    "\n",
    "Types of built-in output sinks.\n",
    "\n",
    "* File sink\n",
    "\t- `<path>` -- path to output directory\n",
    "\t- `<retention>` -- TTL for output files\n",
    "* Kafka sink\n",
    "* Foreach sink\n",
    "* Console sink (for debugging)\n",
    "\t- `<numRows>` -- Number of rows to print every trigger (default: 20)\n",
    "\t- `<truncate>` -- Whether to truncate output if it's too long (default: true)\n",
    "* Memory sink (for debugging)\n",
    "\n",
    "Have to call `.start()` to start the execution of the query. It will return a `StreamingQuery` object which can be used to manage the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output Sinks\n",
    "\n",
    "# File sink - store output to a directory\n",
    "df_stream.writeStream \\\n",
    "\t.format('parquet') \\\n",
    "\t.option('path', 'path/to/destination') \\\n",
    "\t.start()\n",
    "\t\n",
    "# Kafka sink - stores output to one of more Kafka topics\n",
    "df_stream.writeStream \\\n",
    "\t.format('kafka') \\\n",
    "\t.option('kafka.bootstrap.servers', 'host1:port1,host2:port2') \\\n",
    "\t.option('topic', 'updates') \\\n",
    "\t.start()\n",
    "\n",
    "# Foreach sink - Runs arbitrary computation on the records in the output\n",
    "df_stream.writeStream \\\n",
    "\t.foreach() \\\n",
    "\t.start()\n",
    "\n",
    "# Console sink - prints the output to the console/stdout everytime there is a trigger. Only for debugging\n",
    "df_stream.writeStream \\\n",
    "\t.format('console') \\\n",
    "\t.start()\n",
    "\n",
    "# Memory sink - stored in memory as an in-memory table. Only for debugging\n",
    "df_stream.writeStream \\\n",
    "\t.format('memory') \\\n",
    "\t.queryName('tableName') \\\n",
    "\t.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc1f4863a5dfb7e90f6f0646481ce38df4cdefdf4614ba08727c157218b20914"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
