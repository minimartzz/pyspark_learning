{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6a. MLlib Introduction\n",
    "\n",
    "Provides tools like:\n",
    "\n",
    "* ML Algorithms\n",
    "* Featurization: feature extraction, transformation, ...\n",
    "* Pipelines\n",
    "* Persistence: saving and loading algorithms\n",
    "* Utilities: linear algebra, statistics, data handling\n",
    "\n",
    "_Note: RDD-based APIs are considered in maintenance mode. DataFrame-based API is primary_\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "MLlib uses linear algebra packages fro optimised numerical processing, which may call native acceleration libraries that are required. However, native acceleration libraries cannot be distributed together with Spark. Will see a warning message if it is not used\n",
    "\n",
    "For Python you just need NumPy >=1.4\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Statistics\n",
    "\n",
    "MLlib is able to perform basic (and complex) statistics on large data\n",
    "\n",
    "## Correlation\n",
    "\n",
    "Provide the flexibility to calculate pairware correlation among many series. Includes Pearson's and Spearman's correlation algorithms\n",
    "\n",
    "Output here is the correlation matrix for the input Dataset of Vectors using the specified method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      " DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
      "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.40047142, 0.91359586,        nan, 1.        ]])\n",
      "Spearman correlation matrix:\n",
      " DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
      "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"mllib\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]), ), \n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]), ), \n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]), ), \n",
    "\t\t\t\t(Vectors.sparse(4, [(0, 9.0), (3, 1.0)]), ),\n",
    "        ]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "r1 = Correlation.corr(df, 'features').head()\n",
    "print(f\"Pearson correlation matrix:\\n {str(r1[0])}\")\n",
    "\n",
    "r2 = Correlation.corr(df, 'features', 'spearman').head()\n",
    "print(f\"Spearman correlation matrix:\\n {str(r2[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "Determine whether a result is statistically significant. Only supports Pearson's Chi-squared tests for independence.\n",
    "\n",
    "Chi-squared tests: for each feature and label pair, they are converted into a contingency matrix and computed. All labels and features must be categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pValues: [0.6872892787909721,0.6822703303362126]\n",
      "degrees of freedom [2, 3]\n",
      "Statistics: [0.75,1.5]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
    "        (0.0, Vectors.dense(1.5, 20.0)),\n",
    "        (1.0, Vectors.dense(1.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 40.0)),\n",
    "        (1.0, Vectors.dense(3.5, 40.0))]\n",
    "df = spark.createDataFrame(data, ['label', 'features'])\n",
    "\n",
    "r = ChiSquareTest.test(df, 'features', 'label').head()\n",
    "\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(f'degrees of freedom {r.degreesOfFreedom}')\n",
    "print(f\"Statistics: {str(r.statistics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizer\n",
    "\n",
    "Vector column summary statistics (max, min, mean, median, std, variance, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=mllib, master=local[*]) created by getOrCreate at C:\\Users\\Martin Ho\\AppData\\Local\\Temp\\ipykernel_21920\\2451862296.py:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf, SparkContext\n\u001b[0;32m      7\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappName\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaster\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize([Row(weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, features\u001b[38;5;241m=\u001b[39mVectors\u001b[38;5;241m.\u001b[39mdense(\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)),\n\u001b[0;32m     11\u001b[0m                      Row(weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, features\u001b[38;5;241m=\u001b[39mVectors\u001b[38;5;241m.\u001b[39mdense(\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;241m3.0\u001b[39m))])\u001b[38;5;241m.\u001b[39mtoDF()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Create summarizer for multiple metrics 'mean' and 'count'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Martin Ho\\Languages\\python38\\lib\\site-packages\\pyspark\\context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    191\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m     )\n\u001b[1;32m--> 195\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    196\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[0;32m    198\u001b[0m         master,\n\u001b[0;32m    199\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[0;32m    209\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Martin Ho\\Languages\\python38\\lib\\site-packages\\pyspark\\context.py:430\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    427\u001b[0m     callsite \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\u001b[39m.\u001b[39m_callsite\n\u001b[0;32m    429\u001b[0m     \u001b[39m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    431\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot run multiple SparkContexts at once; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mexisting SparkContext(app=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, master=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    433\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m created by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    434\u001b[0m         \u001b[39m%\u001b[39m (\n\u001b[0;32m    435\u001b[0m             currentAppName,\n\u001b[0;32m    436\u001b[0m             currentMaster,\n\u001b[0;32m    437\u001b[0m             callsite\u001b[39m.\u001b[39mfunction,\n\u001b[0;32m    438\u001b[0m             callsite\u001b[39m.\u001b[39mfile,\n\u001b[0;32m    439\u001b[0m             callsite\u001b[39m.\u001b[39mlinenum,\n\u001b[0;32m    440\u001b[0m         )\n\u001b[0;32m    441\u001b[0m     )\n\u001b[0;32m    442\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    443\u001b[0m     SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39m=\u001b[39m instance\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=mllib, master=local[*]) created by getOrCreate at C:\\Users\\Martin Ho\\AppData\\Local\\Temp\\ipykernel_21920\\2451862296.py:5 "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"appName\").setMaster('master')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "df = sc.parallelize([Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
    "                     Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\n",
    "\t\t\t\t\t\t\t\t\t\n",
    "# Create summarizer for multiple metrics 'mean' and 'count'\n",
    "summarizer = Summarizer.metrics('mean', 'count')\n",
    "\n",
    "# Compute statistics for multiple metrics with weight\n",
    "df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)\n",
    "\n",
    "# Compute statistics for multiple metrics without weight\n",
    "df.select(summarizer.summary(df.features)).show(truncate=False)\n",
    "\n",
    "# Compute statistics for single metric 'mean' with weight\n",
    "df.select(summarizer.mean(df.features, df.weight)).show(truncate=False)\n",
    "\n",
    "# Compute statistics for single metrics 'mean' without weight\n",
    "df.select(summarizer.mean(df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data Source\n",
    "\n",
    "How to use data source in ML to load data. Introduce 2 types of data sources beside the general data sources (e.g Parquet, CSV, JSON, etc)\n",
    "\n",
    "1. Image data source\n",
    "2. LIBSVM data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image data source\n",
    "\n",
    "Load image files from a directory. Can eb compressed images into raw representation. DataFrame has 1 `StructType` column: \"image\" containing image data stored as a schema. The Schema:\n",
    "\n",
    "* origin: StringType (represents the file path of the image)\n",
    "* height: IntegerType (height of the image)\n",
    "* width: IntegerType (width of the image)\n",
    "* nChannels: IntegerType (number of image channels)\n",
    "* mode: IntegerType (OpenCV-compatible type)\n",
    "* data: BinaryType (Image bytes in OpenCV-compatible order: row-wise BGR in most cases)\n",
    "\n",
    "Will usually provide Spark SQL data source API to load images as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('image').option('dropInvalid', True).load('data/mllib/images/origin/kittens')\n",
    "df.select('image.origin', 'image.width', 'image.height').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBSVM data source\n",
    "\n",
    "Used to load 'libsvm' type files from a directory. Contains 2 columns: \n",
    "\n",
    "* label: DoubleType (represents the instance label)\n",
    "* features: VectorUDT (represents the feature vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('libsvm').option('numFeatures', '780').load('data/mllib/sample_libsvm_data.txt')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Provide a uniform set of high-level APIs built on top of DataFrames to help users create and tune practical machine learning pipelines.\n",
    "\n",
    "## Main Concept\n",
    "\n",
    "Makes it easier to combine multiple algorithms into a single pipeline or workflow. Pipelines are mostly inspired by scikit-learn projects\n",
    "\n",
    "* `DataFrame`: Uses `DataFrame` from Spark SQL as the ML dataset which can hold different data types\n",
    "* `Transformer`: Transforms one `DataFrame` into another (e.g transforming a `DataFrame` from features into one with predictions)\n",
    "* `Estimator`: Algorithm that's fit on a `DataFrame` to produce a `Transformer` (e.g learning algorithm is the `Estimator` that trains and produces a model) \n",
    "* `Pipelines`: Chain multiple of the above into an ML workflow\n",
    "* `Parameter`: Common API for specifying parameters\n",
    "\n",
    "### DataFrame\n",
    "\n",
    "* `DataFrame` supports many different basic and structured data types from Spark SQL. \n",
    "* Can also use `Vector` type. \n",
    "* Can be created implicitly or explicitly from a regular RDD. \n",
    "* Columns are also named\n",
    "\n",
    "## Pipeline Components\n",
    "\n",
    "### Transformers\n",
    "\n",
    "Abstraction that includes feature transformation and learned models. Generally it implements the method `transform()` that appends new column(s). For example:\n",
    "\n",
    "* Read a column, map it to a new column and output a new `DataFrame` with the mapped column appended\n",
    "* read the column containing feature vectors and predict the label for each feature vector\n",
    "\n",
    "### Estimators\n",
    "\n",
    "Abstracts the concept of a learning algorithm or any algorithm that fits or trains data. Implements a method `fit()` which accepts data and produces a model (that is a `Transformer`)\n",
    "\n",
    "## Properties of pipeline components\n",
    "\n",
    "`Transformer.transform()` and `Estimators.fit()` are stateless. Each instance has a unique ID which is useful in specifying the parameters assocaited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "MLlib representations of workflows, a sequence of `PipelineStages` to be run in order.\n",
    "\n",
    "__How it works__\n",
    "\n",
    "* Each stage is either a `Transformer` or `Estimator`\n",
    "* Input data is transformed as it passes through each stage\n",
    "\t- `Tranformer.transform()`\n",
    "\t- `Estimator.fit()` -> creates a model -> `Transform.transform()`\n",
    "* All `Estimators` in the original Pipeline will become `Transformers` once the model has been fitted\n",
    "* Each stages `transform()` method passes the newly formed dataset onto the next stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_DAG Pipelines_: Pipelines can be structured as Directed Acyclic Graphs (DAG) to be non-linear in fashion, but must be specified in topological order\n",
    "\n",
    "_Runtime checking_: Does not use compile-time checking, only runtime checking before running the pipeline. Done by using the `DataFrames` schema to get a description of the data types of columns to ensure the operations done are valid\n",
    "\n",
    "_Unique Pipeline stages_: Each stage should be a unique instance. No reusing on the same declaration in the same pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "2 ways to pass parameters down to an algorithm:\n",
    "\n",
    "1. Set parameters for an instance (e.g `lr.setMaxIter(10)` will set the `lr.fit()` to use at most 10 iterations)\n",
    "2. Pass a `ParamMap` to the algorithm which will override any parameters previously set by the above method.\n",
    "\n",
    "A single `ParamMap` can be applied to different algorithms each with their own parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters: \n",
      " aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.01)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined) \n",
      "\n",
      "Model 1 was fit using parameters:\n",
      " {Param(parent='LogisticRegression_857d80aa7eaf', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2, Param(parent='LogisticRegression_857d80aa7eaf', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0, Param(parent='LogisticRegression_857d80aa7eaf', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto', Param(parent='LogisticRegression_857d80aa7eaf', name='featuresCol', doc='features column name.'): 'features', Param(parent='LogisticRegression_857d80aa7eaf', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LogisticRegression_857d80aa7eaf', name='labelCol', doc='label column name.'): 'label', Param(parent='LogisticRegression_857d80aa7eaf', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0, Param(parent='LogisticRegression_857d80aa7eaf', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='LogisticRegression_857d80aa7eaf', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LogisticRegression_857d80aa7eaf', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability', Param(parent='LogisticRegression_857d80aa7eaf', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='LogisticRegression_857d80aa7eaf', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent='LogisticRegression_857d80aa7eaf', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LogisticRegression_857d80aa7eaf', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5, Param(parent='LogisticRegression_857d80aa7eaf', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "  .appName('ml example') \\\n",
    "  .getOrCreate()\n",
    "\n",
    "# Prepare training data from a list of (label, features) tuples\n",
    "training = spark.createDataFrame(\n",
    "\t[\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))\n",
    "  ],\n",
    "\t['label', 'features']\n",
    ")\n",
    "\n",
    "# Create a LogsiticRegression instance. This instance is an Estimator\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "print(f\"LogisticRegression parameters: \\n {lr.explainParams()} \\n\")\n",
    "\n",
    "# Learn the LogisticRegression model\n",
    "model1 = lr.fit(training)\n",
    "\n",
    "# model1 is a Model (i.e transformer produced by estimater) able to view parameters used during fit()\n",
    "print(f\"Model 1 was fit using parameters:\\n {model1.extractParamMap()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 was fitted using the following parameters:\n",
      " {Param(parent='LogisticRegression_857d80aa7eaf', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2, Param(parent='LogisticRegression_857d80aa7eaf', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0, Param(parent='LogisticRegression_857d80aa7eaf', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto', Param(parent='LogisticRegression_857d80aa7eaf', name='featuresCol', doc='features column name.'): 'features', Param(parent='LogisticRegression_857d80aa7eaf', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LogisticRegression_857d80aa7eaf', name='labelCol', doc='label column name.'): 'label', Param(parent='LogisticRegression_857d80aa7eaf', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0, Param(parent='LogisticRegression_857d80aa7eaf', name='maxIter', doc='max number of iterations (>= 0).'): 30, Param(parent='LogisticRegression_857d80aa7eaf', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LogisticRegression_857d80aa7eaf', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'myProbability', Param(parent='LogisticRegression_857d80aa7eaf', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='LogisticRegression_857d80aa7eaf', name='regParam', doc='regularization parameter (>= 0).'): 0.1, Param(parent='LogisticRegression_857d80aa7eaf', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LogisticRegression_857d80aa7eaf', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.55, Param(parent='LogisticRegression_857d80aa7eaf', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}\n"
     ]
    }
   ],
   "source": [
    "# specify parameters using Python dictionary in paramMap\n",
    "paramMap = {lr.maxIter: 20}\n",
    "paramMap[lr.maxIter] = 30 # override the first one\n",
    "paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55}) # update with multiple Params\n",
    "\n",
    "# Combine paraMaps which are dictionaries\n",
    "paramMap2 = {lr.probabilityCol: 'myProbability'}\n",
    "paramMapCombined = paramMap.copy()\n",
    "paramMapCombined.update(paramMap2)\n",
    "\n",
    "# Learn a new model using the paramMapCombined parameters\n",
    "model2 = lr.fit(training, paramMapCombined)\n",
    "print(f\"Model 2 was fitted using the following parameters:\\n {model2.extractParamMap()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features=[-1.0,1.5,1.3], label=1.0 -> prob=[0.0570730499357254,0.9429269500642746], prediction=1.0\n",
      "features=[3.0,2.0,-0.1], label=0.0 -> prob=[0.9238521956443227,0.07614780435567725], prediction=0.0\n",
      "features=[0.0,2.2,-1.5], label=1.0 -> prob=[0.10972780286187782,0.8902721971381222], prediction=1.0\n"
     ]
    }
   ],
   "source": [
    "# Prepare some test data\n",
    "test = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# make predictions on test data using Transformer.transform() method\n",
    "# model2 will output a \"myProbability\" column since we changed the name previously\n",
    "prediction = model2.transform(test)\n",
    "result = prediction.select('features', 'label', 'myProbability', 'prediction').collect()\n",
    "\n",
    "for row in result:\n",
    "    print(\"features=%s, label=%s -> prob=%s, prediction=%s\"\n",
    "          % (row.features, row.label, row.myProbability, row.prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 4, spark i j k ) -> prob=[0.6292098489668488,0.37079015103315116], prediction=0.000000\n",
      "( 5, l m n ) -> prob=[0.984770006762304,0.015229993237696027], prediction=0.000000\n",
      "( 6, spark hadoop spark ) -> prob=[0.13412348342566147,0.8658765165743385], prediction=1.000000\n",
      "( 7, apache hadoop ) -> prob=[0.9955732114398529,0.00442678856014711], prediction=0.000000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# Prepare training documents from list of (id, text, label) tuples\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure an ML pipeline with 3 steps\n",
    "# tokenizer, hashingTf and lr\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "hasingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol='features')\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hasingTF, lr])\n",
    "\n",
    "# Fit pipeline to training docuemnts\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# Prepare test documents, unlabeled (io, text) tuples\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents and print column of interest\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select('id', 'text', 'probability', 'prediction').collect()\n",
    "for row in selected:\n",
    "  print(\n",
    "\t\t\"( %d, %s ) -> prob=%s, prediction=%f\" % (row.id, row.text, row.probability, row.prediction) \n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note: Subsequent sections of tutorial explain in detail the different algorithms available in Spark__\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning: Selection and Hyperparameter tuning\n",
    "\n",
    "Describe how to use the tools to help tune ML algorithms and Pipelines. Built-in Cross-Validation and other tools to optimize hyperparameters in algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Tuning can be done for individual `Estimators` or for entire `Pipelines` which include multiple algorithms, featurizations and other steps with multiple hyperparameters, rather than tuning individual elements\n",
    "\n",
    "Tools like `CorssValidator` and `TrainvalidationSplit` require the following:\n",
    "\n",
    "* `Estimator`: algorithm or Pipeline to tune\n",
    "* `ParamMaps`: parameters to choose from (\"Parameter grid\" to search over)\n",
    "* `Evaluator`L metric to measure how well-fitted a model is. Done to held-out test data\n",
    "\n",
    "Works like:\n",
    "\n",
    "1. Split train and test data into multiple separate datasets\n",
    "2. Iterate through a set of `ParamMaps`. Each time, the fitted model is evaluated against the corresponding test set\n",
    "3. Selected model is produced by the best-performing set of parameters\n",
    "\n",
    "__Evaluators__\n",
    "\n",
    "Each 'type' of problem has a corresponding Evaluator. E.g Regression problems = `RegressionEvaluator` | Multi-label classification = `MultilabelClassificationEvaluator`. The default metric can however be changed by the `setMetricName` method in each evaluator\n",
    "\n",
    "__Parameter Grid__\n",
    "\n",
    "Use `ParamGridBuilder` utility to create the parameter grid. By default parameters are evaluated in serial (series), but can be changed to parallel by setting `parallelism` with a value of >=2 before running model selection with `CrossValidator` or `TrainValidationSplit`. Parallelism should be carefully set to maximize performance but not exceed the cluster resources. Larger values does not necessarily increase performance\n",
    "\n",
    "~10 should be sufficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(id=4, text='spark i j k', probability=DenseVector([0.3407, 0.6593]), prediction=1.0)\n",
      "Row(id=5, text='l m n', probability=DenseVector([0.9432, 0.0568]), prediction=0.0)\n",
      "Row(id=6, text='mapreduce spark', probability=DenseVector([0.3449, 0.6551]), prediction=1.0)\n",
      "Row(id=7, text='apache hadoop', probability=DenseVector([0.9563, 0.0437]), prediction=0.0)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Prepare training documents, which are labelled\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0),\n",
    "    (4, \"b spark who\", 1.0),\n",
    "    (5, \"g d a y\", 0.0),\n",
    "    (6, \"spark fly\", 1.0),\n",
    "    (7, \"was mapreduce\", 0.0),\n",
    "    (8, \"e spark program\", 1.0),\n",
    "    (9, \"a e c l\", 0.0),\n",
    "    (10, \"spark compile\", 1.0),\n",
    "    (11, \"hadoop software\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure the ML pipeline\n",
    "tokenizer = Tokenizer(inputCol='text', outputCol='words')\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol='features')\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Use ParamGridBuilder to construct a grid of parameters to search over\n",
    "# With 3 values for hashingTF and 2 values for lr.regParam\n",
    "# grid will have 3 x 2 = 6 parameters settings for CrossValidator to choose from\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "  .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "  .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "  .build()\n",
    "\n",
    "cross_val = CrossValidator(\n",
    "\testimator=pipeline,\n",
    "\testimatorParamMaps=paramGrid,\n",
    "\tevaluator=BinaryClassificationEvaluator(),\n",
    "\tnumFolds=2\n",
    ")\n",
    "\n",
    "# Run cross-validation and choose best set of parameters\n",
    "cv_model = cross_val.fit(training)\n",
    "\n",
    "# Prepare test documents, which are unlabelled\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"mapreduce spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make prediction\n",
    "prediction = cv_model.transform(test)\n",
    "selected_cols = prediction.select('id', 'text', 'probability', 'prediction').collect()\n",
    "for row in selected_cols:\n",
    "  print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "# Prepare training and testing data\n",
    "data = spark.read.format('libsvm').load('data/mllib/sample_linear_regresion_data.txt')\n",
    "train, test = data.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "lr = LinearRegression(maxIter=10)\n",
    "\n",
    "# Create the paramGrid that will host all the different hyperparameters\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "  .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "  .addGrid(lr.fitIntercept, [False, True]) \\\n",
    "  .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "  .build()\n",
    "\n",
    "tvs = TrainValidationSplit(\n",
    "\testimator=lr,\n",
    "\testimatorParamMaps=paramGrid,\n",
    "\tevaluator=RegressionEvaluator(),\n",
    "\ttrainRatio=0.8 # 80% to train, 20% to validate\n",
    ")\n",
    "\n",
    "# Run and choose the best set of parameters\n",
    "model = tvs.fit(train)\n",
    "\n",
    "# Make predictions on test data\n",
    "model.transform(test) \\\n",
    "  .select('features', 'labels', 'prediction') \\\n",
    "  .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc1f4863a5dfb7e90f6f0646481ce38df4cdefdf4614ba08727c157218b20914"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
