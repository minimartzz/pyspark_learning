{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6a. MLlib Introduction\n",
    "\n",
    "Provides tools like:\n",
    "\n",
    "* ML Algorithms\n",
    "* Featurization: feature extraction, transformation, ...\n",
    "* Pipelines\n",
    "* Persistence: saving and loading algorithms\n",
    "* Utilities: linear algebra, statistics, data handling\n",
    "\n",
    "_Note: RDD-based APIs are considered in maintenance mode. DataFrame-based API is primary_\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "MLlib uses linear algebra packages fro optimised numerical processing, which may call native acceleration libraries that are required. However, native acceleration libraries cannot be distributed together with Spark. Will see a warning message if it is not used\n",
    "\n",
    "For Python you just need NumPy >=1.4\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Statistics\n",
    "\n",
    "MLlib is able to perform basic (and complex) statistics on large data\n",
    "\n",
    "## Correlation\n",
    "\n",
    "Provide the flexibility to calculate pairware correlation among many series. Includes Pearson's and Spearman's correlation algorithms\n",
    "\n",
    "Output here is the correlation matrix for the input Dataset of Vectors using the specified method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      " DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
      "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.40047142, 0.91359586,        nan, 1.        ]])\n",
      "Spearman correlation matrix:\n",
      " DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
      "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"mllib\") \\\n",
    "  .getOrCreate()\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]), ), \n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]), ), \n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]), ), \n",
    "\t\t\t\t(Vectors.sparse(4, [(0, 9.0), (3, 1.0)]), ),\n",
    "        ]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "r1 = Correlation.corr(df, 'features').head()\n",
    "print(f\"Pearson correlation matrix:\\n {str(r1[0])}\")\n",
    "\n",
    "r2 = Correlation.corr(df, 'features', 'spearman').head()\n",
    "print(f\"Spearman correlation matrix:\\n {str(r2[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing\n",
    "\n",
    "Determine whether a result is statistically significant. Only supports Pearson's Chi-squared tests for independence.\n",
    "\n",
    "Chi-squared tests: for each feature and label pair, they are converted into a contingency matrix and computed. All labels and features must be categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pValues: [0.6872892787909721,0.6822703303362126]\n",
      "degrees of freedom [2, 3]\n",
      "Statistics: [0.75,1.5]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
    "        (0.0, Vectors.dense(1.5, 20.0)),\n",
    "        (1.0, Vectors.dense(1.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 40.0)),\n",
    "        (1.0, Vectors.dense(3.5, 40.0))]\n",
    "df = spark.createDataFrame(data, ['label', 'features'])\n",
    "\n",
    "r = ChiSquareTest.test(df, 'features', 'label').head()\n",
    "\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(f'degrees of freedom {r.degreesOfFreedom}')\n",
    "print(f\"Statistics: {str(r.statistics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizer\n",
    "\n",
    "Vector column summary statistics (max, min, mean, median, std, variance, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=mllib, master=local[*]) created by getOrCreate at C:\\Users\\Martin Ho\\AppData\\Local\\Temp\\ipykernel_21920\\2451862296.py:5 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf, SparkContext\n\u001b[0;32m      7\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappName\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaster\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize([Row(weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, features\u001b[38;5;241m=\u001b[39mVectors\u001b[38;5;241m.\u001b[39mdense(\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m)),\n\u001b[0;32m     11\u001b[0m                      Row(weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, features\u001b[38;5;241m=\u001b[39mVectors\u001b[38;5;241m.\u001b[39mdense(\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;241m3.0\u001b[39m))])\u001b[38;5;241m.\u001b[39mtoDF()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Create summarizer for multiple metrics 'mean' and 'count'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Martin Ho\\Languages\\python38\\lib\\site-packages\\pyspark\\context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    191\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m     )\n\u001b[1;32m--> 195\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    196\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[0;32m    198\u001b[0m         master,\n\u001b[0;32m    199\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[0;32m    209\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Martin Ho\\Languages\\python38\\lib\\site-packages\\pyspark\\context.py:430\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    427\u001b[0m     callsite \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\u001b[39m.\u001b[39m_callsite\n\u001b[0;32m    429\u001b[0m     \u001b[39m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    431\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot run multiple SparkContexts at once; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mexisting SparkContext(app=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, master=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    433\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m created by \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    434\u001b[0m         \u001b[39m%\u001b[39m (\n\u001b[0;32m    435\u001b[0m             currentAppName,\n\u001b[0;32m    436\u001b[0m             currentMaster,\n\u001b[0;32m    437\u001b[0m             callsite\u001b[39m.\u001b[39mfunction,\n\u001b[0;32m    438\u001b[0m             callsite\u001b[39m.\u001b[39mfile,\n\u001b[0;32m    439\u001b[0m             callsite\u001b[39m.\u001b[39mlinenum,\n\u001b[0;32m    440\u001b[0m         )\n\u001b[0;32m    441\u001b[0m     )\n\u001b[0;32m    442\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    443\u001b[0m     SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39m=\u001b[39m instance\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=mllib, master=local[*]) created by getOrCreate at C:\\Users\\Martin Ho\\AppData\\Local\\Temp\\ipykernel_21920\\2451862296.py:5 "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.stat import Summarizer\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vector\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"appName\").setMaster('master')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "df = sc.parallelize([Row(weight=1.0, features=Vectors.dense(1.0, 1.0, 1.0)),\n",
    "                     Row(weight=0.0, features=Vectors.dense(1.0, 2.0, 3.0))]).toDF()\n",
    "\t\t\t\t\t\t\t\t\t\n",
    "# Create summarizer for multiple metrics 'mean' and 'count'\n",
    "summarizer = Summarizer.metrics('mean', 'count')\n",
    "\n",
    "# Compute statistics for multiple metrics with weight\n",
    "df.select(summarizer.summary(df.features, df.weight)).show(truncate=False)\n",
    "\n",
    "# Compute statistics for multiple metrics without weight\n",
    "df.select(summarizer.summary(df.features)).show(truncate=False)\n",
    "\n",
    "# Compute statistics for single metric 'mean' with weight\n",
    "df.select(summarizer.mean(df.features, df.weight)).show(truncate=False)\n",
    "\n",
    "# Compute statistics for single metrics 'mean' without weight\n",
    "df.select(summarizer.mean(df.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data Source\n",
    "\n",
    "How to use data source in ML to load data. Introduce 2 types of data sources beside the general data sources (e.g Parquet, CSV, JSON, etc)\n",
    "\n",
    "1. Image data source\n",
    "2. LIBSVM data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image data source\n",
    "\n",
    "Load image files from a directory. Can eb compressed images into raw representation. DataFrame has 1 `StructType` column: \"image\" containing image data stored as a schema. The Schema:\n",
    "\n",
    "* origin: StringType (represents the file path of the image)\n",
    "* height: IntegerType (height of the image)\n",
    "* width: IntegerType (width of the image)\n",
    "* nChannels: IntegerType (number of image channels)\n",
    "* mode: IntegerType (OpenCV-compatible type)\n",
    "* data: BinaryType (Image bytes in OpenCV-compatible order: row-wise BGR in most cases)\n",
    "\n",
    "Will usually provide Spark SQL data source API to load images as a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('image').option('dropInvalid', True).load('data/mllib/images/origin/kittens')\n",
    "df.select('image.origin', 'image.width', 'image.height').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIBSVM data source\n",
    "\n",
    "Used to load 'libsvm' type files from a directory. Contains 2 columns: \n",
    "\n",
    "* label: DoubleType (represents the instance label)\n",
    "* features: VectorUDT (represents the feature vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format('libsvm').option('numFeatures', '780').load('data/mllib/sample_libsvm_data.txt')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "Provide a uniform set of high-level APIs built on top of DataFrames to help users create and tune practical machine learning pipelines.\n",
    "\n",
    "## Main Concept\n",
    "\n",
    "Makes it easier to combine multiple algorithms into a single pipeline or workflow. Pipelines are mostly inspired by scikit-learn projects\n",
    "\n",
    "* `DataFrame`: Uses `DataFrame` from Spark SQL as the ML dataset which can hold different data types\n",
    "* `Transformer`: Transforms one `DataFrame` into another (e.g transforming a `DataFrame` from features into one with predictions)\n",
    "* `Estimator`: Algorithm that's fit on a `DataFrame` to produce a `Transformer` (e.g learning algorithm is the `Estimator` that trains and produces a model) \n",
    "* `Pipelines`: Chain multiple of the above into an ML workflow\n",
    "* `Parameter`: Common API for specifying parameters\n",
    "\n",
    "### DataFrame\n",
    "\n",
    "* `DataFrame` supports many different basic and structured data types from Spark SQL. \n",
    "* Can also use `Vector` type. \n",
    "* Can be created implicitly or explicitly from a regular RDD. \n",
    "* Columns are also named\n",
    "\n",
    "## Pipeline Components\n",
    "\n",
    "### Transformers\n",
    "\n",
    "Abstraction that includes feature transformation and learned models. Generally it implements the method `transform()` that appends new column(s). For example:\n",
    "\n",
    "* Read a column, map it to a new column and output a new `DataFrame` with the mapped column appended\n",
    "* read the column containing feature vectors and predict the label for each feature vector\n",
    "\n",
    "### Estimators\n",
    "\n",
    "Abstracts the concept of a learning algorithm or any algorithm that fits or trains data. Implements a method `fit()` which accepts data and produces a model (that is a `Transformer`)\n",
    "\n",
    "## Properties of pipeline components\n",
    "\n",
    "`Transformer.transform()` and `Estimators.fit()` are stateless. Each instance has a unique ID which is useful in specifying the parameters assocaited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "MLlib representations of workflows, a sequence of `PipelineStages` to be run in order.\n",
    "\n",
    "__How it works__\n",
    "\n",
    "* Each stage is either a `Transformer` or `Estimator`\n",
    "* Input data is transformed as it passes through each stage\n",
    "\t- `Tranformer.transform()`\n",
    "\t- `Estimator.fit()` -> creates a model -> `Transform.transform()`\n",
    "* All `Estimators` in the original Pipeline will become `Transformers` once the model has been fitted\n",
    "* Each stages `transform()` method passes the newly formed dataset onto the next stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc1f4863a5dfb7e90f6f0646481ce38df4cdefdf4614ba08727c157218b20914"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
