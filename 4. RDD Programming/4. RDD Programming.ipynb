{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. RDD Programming (Legacy)\n",
    "\n",
    "Spark application consists of a driver program that runs and executes _parallel operations_ on a cluster.\n",
    "\n",
    "* Provides a _resilient distributed database (RDD) which is a collection of elements patitioned across nodes on a cluster that can be operated on in parallel\n",
    "* Persist an RDD in memory to be reused\n",
    "* Automatically recove from node failures\n",
    "\n",
    "_Shared variables_\n",
    "\n",
    "Used when running a function parallel, Spark will copy each vairable used in the function to each task.\n",
    "\n",
    "1. __Broadcast variables__ -- used to cache a value in memory on all nodes\n",
    "2. __Accumulators__ -- variables that are only \"added\" to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linking with Spark\n",
    "\n",
    "* Spark applications can work with C libraries (e.g NumPy)\n",
    "* Need the Spark distribution `bin/spark-submit` script\n",
    "* To access HDFS data, need to use a build of PySpark linked to HDFS\n",
    "* Import the Spark classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Spark\n",
    "\n",
    "Must create a `SparkContext` object, which tells Spark how to access the Cluster. First need to build a `SparkConf` object that contains information about the application.\n",
    "\n",
    "* `SparkConf.setAppName(appName).setMaster(<type>)` -- set configuration\n",
    "\t* `<type>` --  \"local\" if you want to run local testing, \"master\" for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkConf, SparkContext\n\u001b[0;32m      3\u001b[0m conf \u001b[38;5;241m=\u001b[39m SparkConf()\u001b[38;5;241m.\u001b[39msetAppName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappName\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msetMaster(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaster\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Martin Ho\\Languages\\python38\\lib\\site-packages\\pyspark\\context.py:195\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    191\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    192\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    193\u001b[0m     )\n\u001b[1;32m--> 195\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    196\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[0;32m    198\u001b[0m         master,\n\u001b[0;32m    199\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m         udf_profiler_cls,\n\u001b[0;32m    209\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Martin Ho\\Languages\\python38\\lib\\site-packages\\pyspark\\context.py:417\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    416\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> 417\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    418\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\Martin Ho\\Languages\\python38\\lib\\site-packages\\pyspark\\java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(conn_info_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m info:\n\u001b[0;32m    109\u001b[0m     gateway_port \u001b[39m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"appName\").setMaster('master')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resilient Distributed Datasets (RDDs)\n",
    "\n",
    "A fault-tolerant collection of elements that can be operated on in parallel. 2 ways of creating RDDs:\n",
    "\n",
    "1. _Parallelizing_ and existing collection in driver program\n",
    "2. _Referencing_ an external storage system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelized Collections\n",
    "\n",
    "Created by calling `.parallelize()` on an existing collection. Elements of collection are copied to form a distributed dataset.\n",
    "\n",
    "One important parameter is the __number of partitions__ to cut the dataset into. Spark will run one task for each partition. 2-4 partitions for each CPU in cluster. Number of partitions is usually automatic, but can be specified manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "dist_data = sc.parallelize(data)\n",
    "dist_data.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Datasets\n",
    "\n",
    "Create distributed datasets from any storage source supported by Hadoop. (e.g local file system, HDFS, Cassandra, HBase, ...)\n",
    "\n",
    "Notes on reading files:\n",
    "\n",
    "* If using a path on local filesystem, they must also be accessible at the same path on worker nodes. Copy files to workers or use network-mounted shared file system\n",
    "* All file-based input methods, support running on directories and compressed files, and wildcards\n",
    "* Second argument of functions controls the number of partitions on the file\n",
    "\n",
    "Alternative File input methods\n",
    "\n",
    "* Text Files\n",
    "* Pickle Files\n",
    "* Sequence Files\n",
    "* Hadoop Input/Output Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading a textfile \n",
    "dist_file = sc.textFile(\"data.txt\")\n",
    "\n",
    "# Saving and Loading a SequenceFile\n",
    "rdd = sc.parallelize(range(1, 4)).map(lambda x: ('a' * x))\n",
    "rdd.saveAsSequenceFile(\"path/to/file\") # Saving\n",
    "sorted(sc.sequenceFile(\"path/to/file\").collect()) # Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Operations\n",
    "\n",
    "Supports 2 types of operations\n",
    "\n",
    "1. _Transformations_ -- create a new dataset from an existing one\n",
    "\t* All transformations are lazy, only computed when an action requires a result to be returned\n",
    "2. _Actions_ -- returns value to driver program after running a computation on dataset\n",
    "\n",
    "Each RDD may be recomputed each time an action is runned. Able to persist an RDD in memory using `persist` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('data.txt')\n",
    "line_lengths = lines.map(lambda s: len(s))\n",
    "total_length = line_lengths.reduce(lambda a, b: a + b) # computed here\n",
    "\n",
    "line_lengths.persist() # keep this as a persited RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing Functions to Spark\n",
    "\n",
    "3 recommended ways:\n",
    "\n",
    "1. Lambda expressions for simple functions\n",
    "2. local `def` inside the function calling into Spark for longer code\n",
    "3. Top-level functions in a module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longer function\n",
    "if __name__ == \"__main__\":\n",
    "  def myFunc(s):\n",
    "    words = s.split(\"\")\n",
    "    return len(words)\n",
    "  \n",
    "  sc = SparkContext(...)\n",
    "  sc.textFile(\"file.txt\").map(myFunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Closures\n",
    "\n",
    "Understanding the scope and life cycle of variables and methods when executing code across a cluster.\n",
    "\n",
    "Consider the naive RDD element sum below, which may behave differently depending on whether execution is happening within the same JVM. A common example of this is when running Spark in local mode (--master = local[n]) versus deploying a Spark application to a cluster (e.g. via spark-submit to YARN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Don't do this\n",
    "def increment_counter(x):\n",
    "  global counter\n",
    "  counter += x\n",
    "rdd.foreach(increment_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The behavior of the above code is undefined, and may not work as intended. To execute jobs, Spark breaks up the processing of RDD operations into tasks, each of which is executed by an executor. Prior to execution, Spark computes the task’s closure. The closure is those variables and methods which must be visible for the executor to perform its computations on the RDD (in this case foreach()). This closure is serialized and sent to each executor.\n",
    "\n",
    "The variables within the closure sent to each executor are now copies and thus, when counter is referenced within the foreach function, it’s no longer the counter on the driver node. There is still a counter in the memory of the driver node but this is no longer visible to the executors! The executors only see the copy from the serialized closure. Thus, the final value of counter will still be zero since all operations on counter were referencing the value within the serialized closure.\n",
    "\n",
    "In local mode, in some circumstances, the foreach function will actually execute within the same JVM as the driver and will reference the same original counter, and may actually update it.\n",
    "\n",
    "To ensure well-defined behavior in these sorts of scenarios one should use an Accumulator. Accumulators in Spark are used specifically to provide a mechanism for safely updating a variable when execution is split up across worker nodes in a cluster. The Accumulators section of this guide discusses these in more detail.\n",
    "\n",
    "__In general closures - constructs like loops or locally defined methods should not be used to mutate a global state__\n",
    "\n",
    "Some code that work in local mode may not work for distributed (master) mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with key-value pairs\n",
    "\n",
    "A few special operations are available for RDDs of key-value paris. More commonly \"shuffle\" operations, like grouping or aggregating elements by key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"data.txt\")\n",
    "pairs = lines.map(lambda s: (s, 1))\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations & Actions\n",
    "\n",
    "Refer to [here](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations) for list of common transforamtion and actions\n",
    "\n",
    "Note: There are async versions for some actions, e.g `foreachAsync` which will not block the completion of the action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Operations\n",
    "\n",
    "Certain operations trigger a shuffle - Spark's mechanism for re-distributing data so that it's grouped according to partitions. This happens when the data has been altered and each partition needs to be updated in order for the function to output the right data. This involves copying data across executors and machines, meaning the mechanism is costly.\n",
    "\n",
    "Refer to [here](https://spark.apache.org/docs/latest/rdd-programming-guide.html#shuffle-operations) for a detailed explanation of the shuffle mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Persistence\n",
    "\n",
    "Each node stores any partitions of the operation it computes and reuses them in subsequent operations on the dataset. Usually good for iterative algorithms.\n",
    "\n",
    "User `persist()` or `cache()` method, it will be kept in memory on the nodes. Each persisted RDD can be stored in a different _storage level_, done by passing a `StorageLevel` object to the methods.\n",
    "\n",
    "Full list of the storage levels can be found [ here ](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence)\n",
    "\n",
    "__Choosing Storage Levels__\n",
    "\n",
    "* If RDDs fit with default storage (MEMORY_ONLY) leave it as it is. Most efficient method\n",
    "* If not try using MEMORY_ONLY_SER and a fast serialization library to make it more space-efficient, but equally fast\n",
    "* Don't spill into disk unless the computed dataset is expensive or utilizes large amounts of data\n",
    "* Use replicated storage levels for fast fault recovery, not speed\n",
    "\n",
    "__Removing Data__\n",
    "\n",
    "Spark automatically monitors cache usage on each node and drops out old data partitions in a least-recently-used (LRU) fashion. If you would like to manually remove an RDD instead of waiting for it to fall out of the cache, use the `RDD.unpersist()` method. Note that this method does not block by default. To block until resources are freed, specify blocking=true when calling this method.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shared Variables\n",
    "\n",
    "Generally, when Spark operations are runned on remote cluster nodes, the data they act upon are separate copies of all the variables used in the function. These copied variables and the changes made on the function are not propogated back to driver program. \n",
    "\n",
    "However, Spark provides 2 limited types of _shared variables_ that provide this progpogation back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast Variables\n",
    "\n",
    "Keep a read-only variable cached on each machine rather than a shopped copy of it along with the tasks. Spark will automatically broadcast the common data needed by each task at each stage, meaning specifically creating broadcast variables is only useful when tasks across multiple stages needs the same data, or for greater control over the serialization of certain variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcast_var = sc.broadcast([1, 2, 3])\n",
    "broadcast_var.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "call `.unpersist()` to stop release resources of broadcasted variables in that instant. Use `destroy()` to permanently release all resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulators\n",
    "\n",
    "Accumulators are variables that are only “added” to through an associative and commutative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers can add support for new types.\n",
    "\n",
    "As a user, you can create named or unnamed accumulators. Tasks running on a cluster can then add to it using the add method or the += operator. However, they cannot read its value. Only the driver program can read the accumulator’s value, using its value method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using built-in accumulator\n",
    "accum = sc.accumulator(0)\n",
    "sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))\n",
    "\n",
    "accum.value\n",
    "\n",
    "# Create own type of accumulator for different data types\n",
    "from pyspark.accumulators import AccumulatorParam\n",
    "\n",
    "class VectorAccumulatorParam(AccumulatorParam):\n",
    "  def zero(self, initialValue):\n",
    "    return Vector.zeros(initialValue.size)\n",
    "  \n",
    "  def addInPlace(self, v1, v2):\n",
    "    v1 += v2\n",
    "    return v1 \n",
    "\n",
    "vec_accum = sc.accumulator(Vector(...), VectorAccumulatorParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc1f4863a5dfb7e90f6f0646481ce38df4cdefdf4614ba08727c157218b20914"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
