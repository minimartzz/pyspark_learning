{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Spark SQL, DataFrames and Datasets \n",
    "\n",
    "Spark SQL is a Spark module for structured data processing. \n",
    "\n",
    "__Features__\n",
    "\n",
    "* More info about structure of data and computation performed\n",
    "* Better optimization using extra information\n",
    "* Same execution engine is used to compute a result, independent of which API/language used\n",
    "* Able to switch between different APIs to express different transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL\n",
    "\n",
    "Able to execute SQL queries. Results will be returned as a _Dataset/DataFrame_. Can also interact with the SQL interface using the command-line\n",
    "\n",
    "Note: Able to read data from Hive installations\n",
    "\n",
    "## Datasets\n",
    "\n",
    "Distributed collection of data. Available in Java and Scala, not Python or R (but both have benefits of Dataset API available) \n",
    "\n",
    "__Features__\n",
    "\n",
    "* Strong typing and lambda functions\n",
    "* Utilized SQL's optimized execution engine\n",
    "* Manipulated using functional transformations (`map`, `flatMap`, `filter`)\n",
    "\n",
    "## DataFrames\n",
    "\n",
    "A Dataset organized into named columns. Similar to tables in relational databases or dataframes in Python, but with better optimization.\n",
    "\n",
    "__Features__\n",
    "\n",
    "* Constructed from multiple sources: structure data files, tables in Hive, external databases, ...\n",
    "* DataFrame represented by a Dataset of `Rows`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2a. Getting Started\n",
    "\n",
    "## SparkSession\n",
    "\n",
    "Entry point to all functionality in Spark is `SparkSession` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "\t.builder \\\n",
    "\t.appName(\"Python Spark SQL basic example\") \\\n",
    "\t.config(\"spark.some.config.option\", \"some-value\") \\\n",
    "\t.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames\n",
    "\n",
    "Applicatons can create DataFrames from multiple data sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame from JSON file\n",
    "df = spark.read.json('people.json')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untyped Dataset Operations (DataFrame Operations)\n",
    "\n",
    "DataFrames are Dataset Rows in Scala and Java API. \"untyped transformations\" compared to \"typed transformations\".\n",
    "\n",
    "Indexing is some with conventional pandas syntax `df['age']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n",
      "increment age by 1\n",
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n",
      "select people older than 21\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "count people by age\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print DataFrame format \n",
    "df.printSchema()\n",
    "\n",
    "# Selecting columns\n",
    "df.select(\"name\").show()\n",
    "\n",
    "# Applying some filters and functions\n",
    "print(\"increment age by 1\")\n",
    "df.select(df['name'], df['age'] + 1).show()\n",
    "\n",
    "print(\"select people older than 21\")\n",
    "df.filter(df['age'] > 21).show()\n",
    "\n",
    "print(\"count people by age\")\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running SQL Queries Programmatically\n",
    "\n",
    "`sql` function enables applications to run SQL queries and returns results as DataFrames\n",
    "\n",
    "* `.sql(<query>)` -- create sql queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registered as SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Temporary View\n",
    "\n",
    "Session-scoped and will disappear if the session that created it terminates. A global temporary view allows a temporary view that is shared amongst all sessions and kept alive until the Spark application terminates.\n",
    "\n",
    "Global temporary views are stored in a system preserved database `global_temp` and must be called from there.\n",
    "\n",
    "* `.createGlobalTempView()` -- create a view that will persist between sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register as global temporary view\n",
    "df.createGlobalTempView(\"people\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new session to showcase\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets\n",
    "\n",
    "Datasets use a specialized Encoder to serialize the objects for processing or transmitting over the network. Encoder code is dynamically generated for Spark to perform many operations without deserializing the bytes\n",
    "\n",
    "__ONLY IN JAVA AND SCALA__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interoperating with RDDs\n",
    "\n",
    "2 different methods to convert RDDs into Datasets:\n",
    "\n",
    "1. Uses reflection to infer the schema of an RDD that contains specific types of objects - good if you already know the schema\n",
    "2. Through a programmatic interface that allows constructing a schema to be applied to the existing RDD - for unknown column types \n",
    "\n",
    "### Method 1: Reflection\n",
    "\n",
    "Rows are constructed by passing a list of key/value paris as kwargs to the Row class. Keys define column name, types are inferred by sampling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Justin\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "\t.builder \\\n",
    "\t.appName(\"Python Spark SQL basic example\") \\\n",
    "\t.config(\"spark.some.config.option\", \"some-value\") \\\n",
    "\t.getOrCreate()\n",
    "\n",
    "# represents connection to a Spark cluster\n",
    "# sparkContext is a legacy version of SparkSession which is a unified API\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load text file, convert each line into Row\n",
    "lines = sc.textFile(\"people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "# Infer schema and set DataFrame as a table\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can run over DataFrames\n",
    "teenagers = spark.sql(\"SELECT * FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# Results of queries are DataFrame objects\n",
    "teenNames = teenagers.rdd.map(lambda p: \"Name: \" + p.name).collect()\n",
    "for name in teenNames:\n",
    "\tprint(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically Specifying the Schema\n",
    "\n",
    "When a dictionary of kwargs cannot be defined ahead of time (e.g structure of records are all encoded in strings, or text dataset where each user will define their columns differently), a `DataFrame` can be created programmatically:\n",
    "\n",
    "1. Create an RDD of tuples or lists from the original RDD\n",
    "2. Create the schema represented by a `StructType` matching the structure of the tuples or lists created previously\n",
    "3. Apply the schema to the RDD via `createDataFrame` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import data types\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load text file and convert each line to Row\n",
    "lines = sc.textFile(\"people.txt\")\n",
    "parts = lines.map(lambda l: l.split(','))\n",
    "\n",
    "# Convert each line into a tuple\n",
    "people = parts.map(lambda p: (p[0], p[1].strip()))\n",
    "\n",
    "# Schema encoded in a string\n",
    "schemaString = \"name age\"\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply schema to the RDD\n",
    "schemaPeople = spark.createDataFrame(people, schema)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL queries over the DataFrame\n",
    "results = spark.sql(\"SELECT name FROM people\")\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scalar Functions\n",
    "\n",
    "Functions that return a single value per row. Spark SQL supports both built-in and user-defined Scalar functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Functions\n",
    "\n",
    "Functions that return a single value on a group of rows. Supports both built-in and user-defined Aggregate functions.\n",
    "\n",
    "E.g `count()`, `count_distinct()`, `min()`, `max()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc1f4863a5dfb7e90f6f0646481ce38df4cdefdf4614ba08727c157218b20914"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
