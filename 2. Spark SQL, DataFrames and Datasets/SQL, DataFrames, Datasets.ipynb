{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Spark SQL, DataFrames and Datasets \n",
    "\n",
    "Spark SQL is a Spark module for structured data processing. \n",
    "\n",
    "__Features__\n",
    "\n",
    "* More info about structure of data and computation performed\n",
    "* Better optimization using extra information\n",
    "* Same execution engine is used to compute a result, independent of which API/language used\n",
    "* Able to switch between different APIs to express different transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL\n",
    "\n",
    "Able to execute SQL queries. Results will be returned as a _Dataset/DataFrame_. Can also interact with the SQL interface using the command-line\n",
    "\n",
    "Note: Able to read data from Hive installations\n",
    "\n",
    "## Datasets\n",
    "\n",
    "Distributed collection of data. Available in Java and Scala, not Python or R (but both have benefits of Dataset API available) \n",
    "\n",
    "__Features__\n",
    "\n",
    "* Strong typing and lambda functions\n",
    "* Utilized SQL's optimized execution engine\n",
    "* Manipulated using functional transformations (`map`, `flatMap`, `filter`)\n",
    "\n",
    "## DataFrames\n",
    "\n",
    "A Dataset organized into named columns. Similar to tables in relational databases or dataframes in Python, but with better optimization.\n",
    "\n",
    "__Features__\n",
    "\n",
    "* Constructed from multiple sources: structure data files, tables in Hive, external databases, ...\n",
    "* DataFrame represented by a Dataset of `Rows`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2a. Getting Started\n",
    "\n",
    "## SparkSession\n",
    "\n",
    "Entry point to all functionality in Spark is `SparkSession` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "\t.builder \\\n",
    "\t.appName(\"Python Spark SQL basic example\") \\\n",
    "\t.config(\"spark.some.config.option\", \"some-value\") \\\n",
    "\t.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating DataFrames\n",
    "\n",
    "Applicatons can create DataFrames from multiple data sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame from JSON file\n",
    "df = spark.read.json('people.json')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untyped Dataset Operations (DataFrame Operations)\n",
    "\n",
    "DataFrames are Dataset Rows in Scala and Java API. \"untyped transformations\" compared to \"typed transformations\".\n",
    "\n",
    "Indexing is some with conventional pandas syntax `df['age']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n",
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "+-------+\n",
      "\n",
      "increment age by 1\n",
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n",
      "select people older than 21\n",
      "+---+----+\n",
      "|age|name|\n",
      "+---+----+\n",
      "| 30|Andy|\n",
      "+---+----+\n",
      "\n",
      "count people by age\n",
      "+----+-----+\n",
      "| age|count|\n",
      "+----+-----+\n",
      "|  19|    1|\n",
      "|null|    1|\n",
      "|  30|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print DataFrame format \n",
    "df.printSchema()\n",
    "\n",
    "# Selecting columns\n",
    "df.select(\"name\").show()\n",
    "\n",
    "# Applying some filters and functions\n",
    "print(\"increment age by 1\")\n",
    "df.select(df['name'], df['age'] + 1).show()\n",
    "\n",
    "print(\"select people older than 21\")\n",
    "df.filter(df['age'] > 21).show()\n",
    "\n",
    "print(\"count people by age\")\n",
    "df.groupBy(\"age\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running SQL Queries Programmatically\n",
    "\n",
    "`sql` function enables applications to run SQL queries and returns results as DataFrames\n",
    "\n",
    "* `.sql(<query>)` -- create sql queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registered as SQL temporary view\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "sqlDF = spark.sql(\"SELECT * FROM people\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Temporary View\n",
    "\n",
    "Session-scoped and will disappear if the session that created it terminates. A global temporary view allows a temporary view that is shared amongst all sessions and kept alive until the Spark application terminates.\n",
    "\n",
    "Global temporary views are stored in a system preserved database `global_temp` and must be called from there.\n",
    "\n",
    "* `.createGlobalTempView()` -- create a view that will persist between sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Register as global temporary view\n",
    "df.createGlobalTempView(\"people\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new session to showcase\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.people\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Datasets\n",
    "\n",
    "Datasets use a specialized Encoder to serialize the objects for processing or transmitting over the network. Encoder code is dynamically generated for Spark to perform many operations without deserializing the bytes\n",
    "\n",
    "__ONLY IN JAVA AND SCALA__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interoperating with RDDs\n",
    "\n",
    "2 different methods to convert RDDs into Datasets:\n",
    "\n",
    "1. Uses reflection to infer the schema of an RDD that contains specific types of objects - good if you already know the schema\n",
    "2. Through a programmatic interface that allows constructing a schema to be applied to the existing RDD - for unknown column types \n",
    "\n",
    "### Method 1: Reflection\n",
    "\n",
    "Rows are constructed by passing a list of key/value paris as kwargs to the Row class. Keys define column name, types are inferred by sampling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4) (NCS-5CD8391B41.ncs.corp.int-ads executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32md:\\MA Stuff\\Random\\2. Spark SQL, DataFrames and Datasets\\SQL, DataFrames, Datasets.ipynb Cell 16'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/2.%20Spark%20SQL%2C%20DataFrames%20and%20Datasets/SQL%2C%20DataFrames%2C%20Datasets.ipynb#ch0000014?line=8'>9</a>\u001b[0m people \u001b[39m=\u001b[39m parts\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m p: Row(name\u001b[39m=\u001b[39mp[\u001b[39m0\u001b[39m], age\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m(p[\u001b[39m1\u001b[39m])))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/2.%20Spark%20SQL%2C%20DataFrames%20and%20Datasets/SQL%2C%20DataFrames%2C%20Datasets.ipynb#ch0000014?line=10'>11</a>\u001b[0m \u001b[39m# Infer schema and set DataFrame as a table\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/2.%20Spark%20SQL%2C%20DataFrames%20and%20Datasets/SQL%2C%20DataFrames%2C%20Datasets.ipynb#ch0000014?line=11'>12</a>\u001b[0m schemaPeople \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mcreateDataFrame(people)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/2.%20Spark%20SQL%2C%20DataFrames%20and%20Datasets/SQL%2C%20DataFrames%2C%20Datasets.ipynb#ch0000014?line=12'>13</a>\u001b[0m schemaPeople\u001b[39m.\u001b[39mcreateOrReplaceTempView(\u001b[39m\"\u001b[39m\u001b[39mpeople\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/MA%20Stuff/Random/2.%20Spark%20SQL%2C%20DataFrames%20and%20Datasets/SQL%2C%20DataFrames%2C%20Datasets.ipynb#ch0000014?line=14'>15</a>\u001b[0m \u001b[39m# SQL can run over tables\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\P0508911\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:894\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=888'>889</a>\u001b[0m \u001b[39mif\u001b[39;00m has_pandas \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(data, pandas\u001b[39m.\u001b[39mDataFrame):\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=889'>890</a>\u001b[0m     \u001b[39m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=890'>891</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m(SparkSession, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mcreateDataFrame(  \u001b[39m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=891'>892</a>\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=892'>893</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=893'>894</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_dataframe(\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=894'>895</a>\u001b[0m     data, schema, samplingRatio, verifySchema  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=895'>896</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\P0508911\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:934\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=930'>931</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m obj\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=932'>933</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, RDD):\n\u001b[1;32m--> <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=933'>934</a>\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_createFromRDD(data\u001b[39m.\u001b[39;49mmap(prepare), schema, samplingRatio)\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=934'>935</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=935'>936</a>\u001b[0m     rdd, struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_createFromLocal(\u001b[39mmap\u001b[39m(prepare, data), schema)\n",
      "File \u001b[1;32mc:\\Users\\P0508911\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:600\u001b[0m, in \u001b[0;36mSparkSession._createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=595'>596</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=596'>597</a>\u001b[0m \u001b[39mCreate an RDD for DataFrame from an existing RDD, returns the RDD and schema.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=597'>598</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=598'>599</a>\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(schema, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=599'>600</a>\u001b[0m     struct \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inferSchema(rdd, samplingRatio, names\u001b[39m=\u001b[39;49mschema)\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=600'>601</a>\u001b[0m     converter \u001b[39m=\u001b[39m _create_converter(struct)\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=601'>602</a>\u001b[0m     tupled_rdd \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmap(converter)\n",
      "File \u001b[1;32mc:\\Users\\P0508911\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:546\u001b[0m, in \u001b[0;36mSparkSession._inferSchema\u001b[1;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=524'>525</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_inferSchema\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=525'>526</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=526'>527</a>\u001b[0m     rdd: RDD[Any],\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=527'>528</a>\u001b[0m     samplingRatio: Optional[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=528'>529</a>\u001b[0m     names: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=529'>530</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m StructType:\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=530'>531</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=531'>532</a>\u001b[0m \u001b[39m    Infer schema from an RDD of Row, dict, or tuple.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=532'>533</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=543'>544</a>\u001b[0m \u001b[39m    :class:`pyspark.sql.types.StructType`\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=544'>545</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=545'>546</a>\u001b[0m     first \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39;49mfirst()\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=546'>547</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m first:\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/session.py?line=547'>548</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe first row in RDD is empty, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcan not infer schema\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\P0508911\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\rdd.py:1903\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/rdd.py?line=1889'>1890</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfirst\u001b[39m(\u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mRDD[T]\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/rdd.py?line=1890'>1891</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/rdd.py?line=1891'>1892</a>\u001b[0m \u001b[39m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/rdd.py?line=1892'>1893</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/rdd.py?line=1900'>1901</a>\u001b[0m \u001b[39m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/rdd.py?line=1901'>1902</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/rdd.py?line=1902'>1903</a>\u001b[0m     rs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtake(\u001b[39m1\u001b[39;49m)\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/rdd.py?line=1903'>1904</a>\u001b[0m     \u001b[39mif\u001b[39;00m rs:\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/rdd.py?line=1904'>1905</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m rs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\P0508911\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\rdd.py:1883\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/rdd.py?line=1879'>1880</a>\u001b[0m         taken \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/rdd.py?line=1881'>1882</a>\u001b[0m p \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(partsScanned, \u001b[39mmin\u001b[39m(partsScanned \u001b[39m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/rdd.py?line=1882'>1883</a>\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m, takeUpToNumLeft, p)\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/rdd.py?line=1884'>1885</a>\u001b[0m items \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m res\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/rdd.py?line=1885'>1886</a>\u001b[0m partsScanned \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32mc:\\Users\\P0508911\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:1486\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/context.py?line=1483'>1484</a>\u001b[0m mappedRDD \u001b[39m=\u001b[39m rdd\u001b[39m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/context.py?line=1484'>1485</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/context.py?line=1485'>1486</a>\u001b[0m sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mrunJob(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jsc\u001b[39m.\u001b[39;49msc(), mappedRDD\u001b[39m.\u001b[39;49m_jrdd, partitions)\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/context.py?line=1486'>1487</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mc:\\Users\\P0508911\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/java_gateway.py?line=1314'>1315</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/java_gateway.py?line=1315'>1316</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/java_gateway.py?line=1316'>1317</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/java_gateway.py?line=1317'>1318</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/java_gateway.py?line=1319'>1320</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/java_gateway.py?line=1320'>1321</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/java_gateway.py?line=1321'>1322</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/java_gateway.py?line=1323'>1324</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/java_gateway.py?line=1324'>1325</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\P0508911\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/utils.py?line=187'>188</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/utils.py?line=188'>189</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/utils.py?line=189'>190</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/utils.py?line=190'>191</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/pyspark/sql/utils.py?line=191'>192</a>\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\P0508911\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/protocol.py?line=323'>324</a>\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/protocol.py?line=324'>325</a>\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/protocol.py?line=325'>326</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/protocol.py?line=326'>327</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/protocol.py?line=327'>328</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/protocol.py?line=328'>329</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/protocol.py?line=329'>330</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/protocol.py?line=330'>331</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/P0508911/AppData/Local/Programs/Python/Python310/lib/site-packages/py4j/protocol.py?line=331'>332</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 4) (NCS-5CD8391B41.ncs.corp.int-ads executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:189)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:164)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:176)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# represents connection to a Spark cluster\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Load text file, convert each line into Row\n",
    "lines = sc.textFile(\"people.txt\")\n",
    "parts = lines.map(lambda l: l.split(\",\"))\n",
    "people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))\n",
    "\n",
    "# Infer schema and set DataFrame as a table\n",
    "schemaPeople = spark.createDataFrame(people)\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can run over tables\n",
    "teenagers = spark.sql(\"SELECT * FROM people WHERE age >= 13 AND age <= 19\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be53bca86783c5ebc1c4a1f19d1efb4121d14fd473a368b8237af582f8e0193e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
