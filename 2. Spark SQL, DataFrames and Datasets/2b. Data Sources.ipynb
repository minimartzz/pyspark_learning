{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2b. Data Sources\n",
    "\n",
    "Spark SQL supports operations on variety of data sources through the DataFrame interface. Can perform relational transformations or create temporary views - allowing SQL queries to be run over the data.\n",
    "\n",
    "Section will describe general methods for loading and saving data using Spark Data Sources and options for built-in data sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Load/Save Functions\n",
    "\n",
    "Default data source (`parquet`) will be used for all operations\n",
    "\n",
    "* `spark.read.load(<source>)` -- load data \n",
    "* `df.select(<cols>).write.save(<path>)` -- save data\n",
    "\n",
    "__Apache parquet__\n",
    "\n",
    "File format to support fast data processing for complex data. Has better compression, speed and performance compared to other file formats, therefore it's useful for large amounts of data\n",
    "\n",
    "Features:\n",
    "\n",
    "* Columnar - data entries are stored in columns instead of rows\n",
    "* Open Source - free to use and open source under Apache Hadoop\n",
    "* Self-describing - contains metadata like schema and structure; has standards used for accessing each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"data sources\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "df = spark.read.load(\"users.parquet\")\n",
    "df.show()\n",
    "\n",
    "# Save data\n",
    "# df.select('name', 'favorite_color').write.save('saved_data/namesAndFavColour.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Specifying Options\n",
    "\n",
    "Manually specify extra options to pass to the data source. Data sources can generally be converted into other types of data using short names (e.g json, parquet, orc, libsvm, csv, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading json file\n",
    "df = spark.read.load(\"people.json\", format='json')\n",
    "\n",
    "# Saving in parquet format\n",
    "# df.select('name', 'age').write.save(\"nameAndAges.parquet\", format='parquet')\n",
    "\n",
    "# Loading csv\n",
    "df_csv = spark.read.load('people.csv', format='csv', sep=';', inferSchema='true', header='true')\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra options also in write operations. The below example will create a bloom filter and use dictionary encodings only for the `favorite_color` column on an ORC data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "df = spark.read.orc('users.orc')\n",
    "(df.write.format('orc')\n",
    "\t.option('orc.bloom.filter.columns', 'favorite_color')\n",
    "\t.option('orc.dictionary.key.threshold', '1.0')\n",
    "\t.option('orc.column.encoding.direct', 'name')\n",
    "\t.save('users_with_options.orc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run SQL on files directly\n",
    "\n",
    "Instead of using read API to load files, you can immediately run SQL queries by wrapping file path in \\`\\`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM parquet.`users.parquet`\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Modes\n",
    "\n",
    "Optinonal parameter to specify how to handle existing data if present. Save modes do not utilize any locking and are not atomic. When performing `Overwrite`, the data is deleted before writing out new data.\n",
    "\n",
    "`spark.select(<cols>).write.format(<format>).save(<path>, <mode>)`\n",
    "\n",
    "* `error` or `errorifexists` -- throw exception if file already exists\n",
    "* `append` -- append new data source to existing data, headers must match\n",
    "* `overwrite` -- if table exists, overwrite the table\n",
    "* `ignore` -- save operation is expected not to change the existing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic File Source Option\n",
    "\n",
    "Options are only used for file-based sourceS: _parquet, orc, avro, json, csv, text_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignoring Corrupt/Missing Files\n",
    "\n",
    "Spark jobs will continue to run when encountering corrupt/missing files. Contents that have been read will still be returned\n",
    "\n",
    "* `spark.sql.files.ignoreCorruptFiles` - ignore corrupt files\n",
    "* `spark.sql.files.ignoreMissingFiles` - ignore missing files (missing files are generally deleted files under the directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"data sources\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         file|\n",
      "+-------------+\n",
      "|file1.parquet|\n",
      "|file2.parquet|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Enable ignore corrupt files\n",
    "spark.sql('set spark.sql.files.ignoreCorruptFiles=true')\n",
    "\n",
    "# json file will be ignored \n",
    "test_corrupt_df = spark.read.parquet(\n",
    "\t\"dir1/\",\n",
    "\t\"dir1/dir2\"\n",
    ")\n",
    "test_corrupt_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path Global Filter\n",
    "\n",
    "Only include files with file names that match a pattern. Does not change the behaviour of partition discovery\n",
    "\n",
    "It is called a method in the `.load()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         file|\n",
      "+-------------+\n",
      "|file1.parquet|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load('dir1', format='parquet', pathGlobFilter='*.parquet')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Lookup\n",
    "\n",
    "Recursively load files and it disables parition inferring. Default to `False`. If data source specifies the `partitionSpec` when this option is True, an exception will be thrown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         file|\n",
      "+-------------+\n",
      "|file1.parquet|\n",
      "|file2.parquet|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recursive_loaded_df = spark.read.format('parquet') \\\n",
    "\t.option('recursiveFileLookup', 'true') \\\n",
    "\t.load('dir1')\n",
    "recursive_loaded_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modification Time Path Filters\n",
    "\n",
    "Options to achieve greater granularity over which files may load during a Spark batch query.\n",
    "\n",
    "* `modifiedBefore`: an optional timestamp to include files with modification time occurring before a spacified time\n",
    "* `modifiedAfter`: an optional timestamp to include files with modification time occurring after a spacified time\n",
    "\n",
    "Timestamp format: __YYYY-MM-DDTHH:mm:ss__ (e.g 2022-09-19T12:00:14)\n",
    "\n",
    "When timezone option not specified, it will default to Spark session timezone\n",
    "\n",
    "Note: Structure Streaming file sources don't support these options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|         file|\n",
      "+-------------+\n",
      "|file1.parquet|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# modifiedBefore a specific date\n",
    "df = spark.read.load('dir1', format='parquet', modifiedBefore='2050-07-01T08:30:00')\n",
    "df.show()\n",
    "\n",
    "# modifiedAfter a specific date\n",
    "df = spark.read.load('dir1', format='parquet', modifiedAfter='2050-06-01T08:30:00')\n",
    "df.show() # empty return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet is a columnar format file format used generally for big data operations. Spark SQL supports both reading and writing parquet files that automatically preserve the schema of the original data. All columns are automatically converted to nullable for compatibility reasons when reading a parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data Programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"parquet files\").getOrCreate()\n",
    "\n",
    "df_people = spark.read.json('people.json')\n",
    "\n",
    "# DataFrames saved as parquet files\n",
    "df_people.write.parquet('people.parquet')\n",
    "\n",
    "# Read in the newly created parquet file\n",
    "parquet_people = spark.read.parquet('people.parquet')\n",
    "\n",
    "# parquet files can also be used to create a temporary view for SQL queries\n",
    "parquet_people.createOrReplaceTempView('parquet_people')\n",
    "teenagers = spark.sql(\"SELECT name FROM parquet_people WHERE age >= 13 AND age <= 19\")\n",
    "teenagers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Discovery\n",
    "\n",
    "Table partitioning is a common optimization approach used in systems. In a partitioned table, data is usually stored in different directories, with partitioning column values encoded in the path of each directory. All built-in file sources are able to discover and infer partitioning information automatically.\n",
    "\n",
    "__Features__\n",
    "\n",
    "* Data types are automatically inferred when importing\n",
    "\t- Manual specification can also be done by changing `spark.sql.sources.partitionColumnTypeInference.enabled` to `False` and will default to `string` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Merging\n",
    "\n",
    "Gradually add more columns to schemas when needed. Users may end up with multiple _parquet_ files with different but mutually compatible schemas. Parquet data source will automatically detect and merge schemas of these files\n",
    "\n",
    "Schema merging is a relatively expensive operation, so it is turned off by default. To enable:\n",
    "\n",
    "1. Set data source option `mergeSchema` to true when reading Parquet files\n",
    "2. Set global SQL option `spark.sql.parquet.mergeSchema` to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Create DataFrame and store into partition directory\n",
    "sc = spark.sparkContext\n",
    "\n",
    "df_squares = spark.createDataFrame(sc.parallelize(range(1, 6)).map(lambda i: Row(single=i, double=i**2)))\n",
    "df_squares.write.parquet('saved_data/test_table/key=1')\n",
    "\n",
    "# Create another DataFrame in a new parition directory\n",
    "# add a new column and drop an existing column\n",
    "df_cubes = spark.createDataFrame(sc.parallelize(range(6, 11)).map(lambda i: Row(single=i, triple=i**3)))\n",
    "df_cubes.write.parquet('saved_data/test_table/key=2')\n",
    "\n",
    "# Read paritioned table\n",
    "df_merged = spark.read \\\n",
    "\t.option('mergedSchema', 'true') \\\n",
    "\t.parquet('saved_data/test_table')\n",
    "df_merged.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hive metastore Parquet table conversion\n",
    "\n",
    "Spark SQL will try to use own parquet support instead of Hive SerDe for better performance when reading from Hive metastore. Controlled by `spark.sql.hive.convertMetastoreParquet` turned on by default\n",
    "\n",
    "__Schema Reconciliation__\n",
    "\n",
    "Differences:\n",
    "\n",
    "1. Hive is case insensitive. Parquet is not\n",
    "2. Hive considers all columns nullable. In Parquet nullability is important\n",
    "\n",
    "Rules:\n",
    "\n",
    "1. Fields that have the same name in both schemas must have the same data type regardless of nullability. Nullability must be respected for parquet side as well\n",
    "2. Reconciled schema contains only fields defined in Hive metastore schema\n",
    "\t* fields that only appear in parquet schema are dropped\n",
    "\t* fields that only appear in Hive metastore are added as nullable fields\n",
    "\n",
    "__Metadata Refreshing__\n",
    "\n",
    "Spark SQL caches parquet metadata for better performance, including converted tables. Need to manually referesh tables if they are updated by Hive or external tools\n",
    "\n",
    "* `spark.catalog.refreshTable(<table>)` -- refresh table in existing SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colmnar Encryption\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc1f4863a5dfb7e90f6f0646481ce38df4cdefdf4614ba08727c157218b20914"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
