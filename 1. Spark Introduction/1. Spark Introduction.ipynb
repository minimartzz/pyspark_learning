{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction to Spark\n",
    "\n",
    "Quick Introduction to Spark. Introduce the API through the interactive shell, then write some simple applications in Python. \n",
    "\n",
    "__Requirements__\n",
    "\n",
    "Download a packaged release of Spark from [here](https://www.apache.org/dyn/closer.lua/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz). Will need to extract the tarball file first.\n",
    "\n",
    "Note: Resilient Distributed Dataset (RDD) which was the default programming interface has been replaced by `Dataset`, which is strongly-typed like an RDD, but with richer optimizations. Recommend to use Dataset which has better performance than RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is E4B7-2A40\n",
      "\n",
      " Directory of d:\\MA Stuff\\Random\n",
      "\n",
      "09/09/2022  11:27 AM    <DIR>          .\n",
      "09/09/2022  11:27 AM    <DIR>          ..\n",
      "09/09/2022  10:29 AM               792 1. Spark Introduction.ipynb\n",
      "09/09/2022  10:29 AM               113 test.py\n",
      "               2 File(s)            905 bytes\n",
      "               2 Dir(s)  92,224,647,168 bytes free\n"
     ]
    }
   ],
   "source": [
    "# Installing Spark tarball file - Windows\n",
    "!tar -xzvf <filename.tar.gz>\n",
    "\n",
    "# Starting the interactive shell - navigate to folder\n",
    "!./bin\n",
    "!pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1a. Interactive Analysis with Spark Shell\n",
    "\n",
    "Spark's primary abstraction is a distributed collection of items called a Dataset.\n",
    "\n",
    "* Can be created from Hadoop InputFormats or transformaing any Datasets\n",
    "* Dataset doesn't need to be strongly-typed\n",
    "* Datasets are `Dataset[Row]` called `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!textFile = spark.read.text(\"ps_README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting information about DataFrame\n",
    "!textFile.count()\n",
    "!textFile.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for subset of lines in file\n",
    "!linesWithSpark = textFile.filter(textFile.value.contains(\"Spark\"))\n",
    "!textFile.filter(textFile.value.contains(\"Spark\")).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Dataset Operations\n",
    "\n",
    "Dataset actions and transformations can be used for more complex computations. We'll start using Python here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(numWords)=16)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "textFile = spark.read.text(\"ps_README.md\")\n",
    "lines_with_spark = textFile.filter(textFile.value.contains(\"Spark\"))\n",
    "\n",
    "textFile.select(size(split(textFile.value, \"\\s+\")).name(\"numWords\")).agg(max(col(\"numWords\"))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above \n",
    "\n",
    "1. Maps the number of words in a line to an integer value and aliases it as \"numWords\"\n",
    "2. Creates a new DataFrame\n",
    "3. Runs the `agg` command to find the largest number of words in a line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                word|count|\n",
      "+--------------------+-----+\n",
      "|          [![PySpark|    1|\n",
      "|              online|    1|\n",
      "|              graphs|    1|\n",
      "|Build](https://gi...|    1|\n",
      "|                 API|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MapReduce in Spark\n",
    "wordCounts = textFile.select(explode(split(textFile.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count()\n",
    "wordCounts.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above shows the number of times a word appears in the document:\n",
    "\n",
    "1. `split` splits each row into individual items in an array and `explode` makes each element in the array (word) a new row\n",
    "2. Each word is then grouped together using `groupBy`\n",
    "3. Count the number of words that are \"grouped\" together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Caching\n",
    "\n",
    "Supports pulling data into cluster-wide in-memory cache. Useful when querying hot datasets, or iterative algorithms like PageRank\n",
    "\n",
    "_Commands_\n",
    "\n",
    "* `.cache` - stores DataFrame in cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_with_spark.cache()\n",
    "lines_with_spark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c. Self-Contained Applications\n",
    "\n",
    "Self-contained applications are single, installable bundle that contains application and a set of dependencies needed to run the application. When installed it behaves the same way as a native application\n",
    "\n",
    "To build a packaged PySpark application, add this to the setup.py file:\n",
    "\n",
    "```\n",
    "install_requires=[\n",
    "\t'pyspark=3.3.0'\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines with a: 71, lines with b: 38\n"
     ]
    }
   ],
   "source": [
    "# Simple PySpark application\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "logFile = \"ps_README.md\"\n",
    "spark = SparkSession.builder.appName(\"SimpleApp\").getOrCreate()\n",
    "logData = spark.read.text(logFile).cache()\n",
    "\n",
    "numAs = logData.filter(logData.value.contains('a')).count()\n",
    "numBs = logData.filter(logData.value.contains('b')).count()\n",
    "\n",
    "print(f\"Lines with a: {numAs}, lines with b: {numBs}\")\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run application on Python interpreter\n",
    "!python SimpleApp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For additional dependencies, they can be added to `spark-submit` through its `--py-files` argument by packaging them into a .zip file"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be53bca86783c5ebc1c4a1f19d1efb4121d14fd473a368b8237af582f8e0193e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
